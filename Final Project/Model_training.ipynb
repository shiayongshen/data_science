{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbe1b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307633f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d03f31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:23:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Beverages       0.10      0.10      0.10       308\n",
      "                          Butchers       0.13      0.13      0.13       305\n",
      "Computers and electric accessories       0.14      0.14      0.14       305\n",
      "     Electric household essentials       0.12      0.12      0.12       312\n",
      "                              Food       0.10      0.10      0.10       313\n",
      "                         Furniture       0.11      0.11      0.11       310\n",
      "                     Milk Products       0.13      0.13      0.13       308\n",
      "                        Patisserie       0.12      0.11      0.11       299\n",
      "\n",
      "                          accuracy                           0.12      2460\n",
      "                         macro avg       0.12      0.12      0.12      2460\n",
      "                      weighted avg       0.12      0.12      0.12      2460\n",
      "\n",
      "\n",
      "é¡žåˆ¥ç·¨ç¢¼æ˜ å°„:\n",
      "0: Beverages\n",
      "1: Butchers\n",
      "2: Computers and electric accessories\n",
      "3: Electric household essentials\n",
      "4: Food\n",
      "5: Furniture\n",
      "6: Milk Products\n",
      "7: Patisserie\n",
      "Top-1 æº–ç¢ºçŽ‡: 0.1167\n",
      "Top-3 æº–ç¢ºçŽ‡: 0.3817\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('retail_store_sales_cleaned_feature_engineering.csv')\n",
    "# åŽŸå§‹æŽ’åº\n",
    "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
    "df = df.sort_values(['Customer ID', 'Transaction Date']).reset_index(drop=True)\n",
    "\n",
    "# å»ºç«‹ Category LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Category_ID'] = le.fit_transform(df['Category'])\n",
    "\n",
    "# å»ºç«‹åµŒå…¥çŸ©é™£ï¼ˆä¾‹å¦‚æ¯å€‹é¡žåˆ¥å°æ‡‰ 8 ç¶­å‘é‡ï¼‰\n",
    "NUM_CATEGORIES = df['Category_ID'].nunique()\n",
    "EMBEDDING_DIM = 16\n",
    "embedding_layer = nn.Embedding(NUM_CATEGORIES, EMBEDDING_DIM)\n",
    "\n",
    "# è¨˜éŒ„æ‰€æœ‰ç‰¹å¾µå‘é‡\n",
    "feature_rows = []\n",
    "\n",
    "# è¨­å®š Nï¼šè€ƒæ…®éŽåŽ» N ç­†è³¼è²·è¡Œç‚º\n",
    "N = 10\n",
    "\n",
    "# åœ¨åŽŸæœ‰ç‰¹å¾µåŸºç¤Žä¸Šï¼Œæ–°å¢žæ›´å¤šæ™‚é–“åºåˆ—ç‰¹å¾µ\n",
    "for cust_id, cust_df in df.groupby('Customer ID'):\n",
    "    cust_df = cust_df.sort_values('Transaction Date')\n",
    "    for i in range(N, len(cust_df) - 1):\n",
    "        history = cust_df.iloc[i-N:i]\n",
    "        current = cust_df.iloc[i]\n",
    "        next_row = cust_df.iloc[i + 1]\n",
    "\n",
    "        # åµŒå…¥ç‰¹å¾µ\n",
    "        cat_ids = torch.tensor(history['Category_ID'].tolist(), dtype=torch.long)\n",
    "        embedded = embedding_layer(cat_ids)\n",
    "        embedded_flat = embedded.flatten().detach().numpy()\n",
    "\n",
    "        # å¢žå¼·çš„ç‰¹å¾µå·¥ç¨‹\n",
    "        row = {\n",
    "            # åŽŸæœ‰ç‰¹å¾µ\n",
    "            'TotalSpent_Mean': history['Total Spent'].mean(),\n",
    "            'TotalSpent_Std': history['Total Spent'].std(),\n",
    "            'TotalSpent_Last': history['Total Spent'].iloc[-1],\n",
    "            'Quantity_Mean': history['Quantity'].mean(),\n",
    "            'Quantity_Sum': history['Quantity'].sum(),\n",
    "            'Discount_Used_Count': history['Disc_True'].sum(),\n",
    "            'Discount_Rate': history['Disc_True'].mean(),\n",
    "            \n",
    "            # æ™‚é–“ç‰¹å¾µ\n",
    "            'Recency_Days': (current['Transaction Date'] - history['Transaction Date'].max()).days,\n",
    "            'Frequency': len(history),\n",
    "            'Days_Between_Purchases': history['Transaction Date'].diff().dt.days.mean(),\n",
    "            \n",
    "            # é¡žåˆ¥åºåˆ—ç‰¹å¾µ\n",
    "            'Most_Frequent_Category': history['Category'].mode().iloc[0] if not history['Category'].mode().empty else 'Unknown',\n",
    "            'Category_Diversity': history['Category'].nunique(),\n",
    "            'Last_Category': history['Category'].iloc[-1],\n",
    "            'Category_Repeat_Rate': (history['Category'] == history['Category'].iloc[-1]).mean(),\n",
    "            \n",
    "            # ç¾åœ¨çš„äº¤æ˜“ç‰¹å¾µ\n",
    "            'Is_Weekend': current['Is_Weekend'],\n",
    "            'Is_Holiday': current['Is_Holiday'],\n",
    "            'Is_NonWorkday': current['Is_NonWorkday'],\n",
    "            'PM_Credit Card': current['PM_Credit Card'],\n",
    "            'PM_Digital Wallet': current['PM_Digital Wallet'],\n",
    "            'Loc_Online': current['Loc_Online'],\n",
    "            'Disc_True': current['Disc_True'],\n",
    "            'Disc_Unknown': current['Disc_Unknown'],\n",
    "            'Current_Category': current['Category'],\n",
    "            'Next_Category': next_row['Category']\n",
    "        }\n",
    "\n",
    "        # æ‹¼æŽ¥åµŒå…¥ç‰¹å¾µ\n",
    "        for j, val in enumerate(embedded_flat):\n",
    "            row[f'Embed_{j}'] = val\n",
    "\n",
    "        feature_rows.append(row)\n",
    "        \n",
    "\n",
    "feature_df = pd.DataFrame(feature_rows).fillna(0)\n",
    "\n",
    "# å°ç›®æ¨™è®Šæ•¸é€²è¡Œæ¨™ç±¤ç·¨ç¢¼\n",
    "y_encoder = LabelEncoder()\n",
    "feature_df['Next_Category_Encoded'] = y_encoder.fit_transform(feature_df['Next_Category'])\n",
    "\n",
    "# å°å­—ä¸²é¡žåž‹çš„ç‰¹å¾µé€²è¡Œç·¨ç¢¼\n",
    "category_encoder = LabelEncoder()\n",
    "feature_df['Most_Frequent_Category_Encoded'] = category_encoder.fit_transform(feature_df['Most_Frequent_Category'])\n",
    "\n",
    "last_category_encoder = LabelEncoder()\n",
    "feature_df['Last_Category_Encoded'] = last_category_encoder.fit_transform(feature_df['Last_Category'])\n",
    "\n",
    "# ç§»é™¤åŽŸå§‹å­—ä¸²ç‰¹å¾µï¼Œä¿ç•™ç·¨ç¢¼å¾Œçš„ç‰¹å¾µ\n",
    "feature_df = feature_df.drop(columns=['Most_Frequent_Category', 'Last_Category'])\n",
    "\n",
    "# One-hot encode ç¾åœ¨è³¼è²·çš„é¡žåˆ¥\n",
    "X = pd.get_dummies(feature_df.drop(columns=['Next_Category', 'Next_Category_Encoded']), columns=['Current_Category'])\n",
    "y = feature_df['Next_Category_Encoded']  # ä½¿ç”¨ç·¨ç¢¼å¾Œçš„æ¨™ç±¤\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost æ¨¡åž‹è¨“ç·´\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# é æ¸¬èˆ‡è©•ä¼°\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# å°‡é æ¸¬çµæžœè½‰æ›å›žåŽŸå§‹é¡žåˆ¥åç¨±ä»¥ä¾¿æŸ¥çœ‹\n",
    "y_test_labels = y_encoder.inverse_transform(y_test)\n",
    "y_pred_labels = y_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "\n",
    "# é¡¯ç¤ºé¡žåˆ¥æ˜ å°„\n",
    "print(\"\\né¡žåˆ¥ç·¨ç¢¼æ˜ å°„:\")\n",
    "for i, category in enumerate(y_encoder.classes_):\n",
    "    print(f\"{i}: {category}\")\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# ä½¿ç”¨ sklearn çš„ top_k_accuracy_score\n",
    "top1_acc = top_k_accuracy_score(y_test, y_pred_proba, k=1)\n",
    "top3_acc = top_k_accuracy_score(y_test, y_pred_proba, k=3)\n",
    "\n",
    "print(f\"Top-1 æº–ç¢ºçŽ‡: {top1_acc:.4f}\")\n",
    "print(f\"Top-3 æº–ç¢ºçŽ‡: {top3_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414510f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰€æœ‰é¡§å®¢ä¸­æœ€å¤šçš„è³¼è²·æ¬¡æ•¸æ˜¯ï¼š544\n"
     ]
    }
   ],
   "source": [
    "# è¨ˆç®—æ¯ä½é¡§å®¢çš„è³¼è²·æ¬¡æ•¸\n",
    "cust_counts = df.groupby('Customer ID').size()\n",
    "\n",
    "# æœ€å¤§è³¼è²·æ¬¡æ•¸\n",
    "max_n = cust_counts.max()\n",
    "print(f\"âœ… æ‰€æœ‰é¡§å®¢ä¸­æœ€å¤šçš„è³¼è²·æ¬¡æ•¸æ˜¯ï¼š{max_n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8421b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ ä½ æœ€å¤šå¯ä»¥è¨­ N ç‚ºï¼š543\n"
     ]
    }
   ],
   "source": [
    "purchase_counts = df.groupby('Customer ID').size()\n",
    "max_available_N = purchase_counts.max() - 1  # æœ€é•·å¯ç”¢ç”Ÿåºåˆ—çš„é•·åº¦\n",
    "\n",
    "print(f\"ðŸ”§ ä½ æœ€å¤šå¯ä»¥è¨­ N ç‚ºï¼š{max_available_N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Total training samples created: 232\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da46a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Total training samples created: 12550\n",
      "\n",
      "ðŸš€ Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     95\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m---> 96\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[1;32m     97\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "PAD_TOKEN = -1\n",
    "df = pd.read_csv('retail_store_sales_cleaned_feature_engineering.csv')\n",
    "# =======================\n",
    "# è³‡æ–™è™•ç†ï¼šçµ„æˆåºåˆ—è³‡æ–™\n",
    "# =======================\n",
    "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
    "df = df.sort_values(['Customer ID', 'Transaction Date']).reset_index(drop=True)\n",
    "\n",
    "# ç·¨ç¢¼é¡žåˆ¥\n",
    "le = LabelEncoder()\n",
    "df['Category_ID'] = le.fit_transform(df['Category'])\n",
    "num_classes = df['Category_ID'].nunique()\n",
    "\n",
    "\n",
    "\n",
    "PAD_TOKEN = df['Category_ID'].nunique()  # é¡žåˆ¥æ•¸ï¼Œä¿è­‰ PAD ä¸é‡è¤‡\n",
    "N = 500  # å›ºå®šåºåˆ—é•·åº¦\n",
    "seq_data = []\n",
    "\n",
    "for cust_id, cust_df in df.groupby('Customer ID'):\n",
    "    category_ids = cust_df['Category_ID'].tolist()\n",
    "    T = len(category_ids)\n",
    "\n",
    "    for i in range(1, T):  # æ¯ä¸€ç­†è¦é æ¸¬ç¬¬ i ç­†ï¼ˆå¾žç¬¬ 1 ç­†é–‹å§‹ï¼‰\n",
    "        history = category_ids[:i]           # é¡§å®¢ç¬¬ i ç­†å‰çš„æ‰€æœ‰ç´€éŒ„\n",
    "        padded = [PAD_TOKEN] * max(0, N - len(history)) + history[-N:]  # è£œ PAD è‡³é•·åº¦ N\n",
    "        label = category_ids[i]              # ç¬¬ i ç­†çš„é¡žåˆ¥å°±æ˜¯ label\n",
    "        seq_data.append((padded, label))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"ðŸ“Š Total training samples created: {len(seq_data)}\")\n",
    "# åˆ†è¨“ç·´æ¸¬è©¦é›†\n",
    "X_seq, y_seq = zip(*seq_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(X_seq), list(y_seq), test_size=0.1, stratify=y_seq)\n",
    "\n",
    "# =======================\n",
    "# å»ºç«‹ Dataset & DataLoader\n",
    "# =======================\n",
    "class PurchaseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = PurchaseDataset(X_train, y_train)\n",
    "test_ds = PurchaseDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "# =======================\n",
    "# å»ºç«‹ LSTM æ¨¡åž‹\n",
    "# =======================\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, num_classes, emb_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_classes + 1, embedding_dim=32, padding_idx=PAD_TOKEN)\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch, seq_len, emb_dim)\n",
    "        _, (h_n, _) = self.lstm(x)  # åªå–æœ€å¾Œä¸€å±¤ hidden\n",
    "        out = self.fc(h_n.squeeze(0))  # (batch, num_classes)\n",
    "        return out\n",
    "\n",
    "# =======================\n",
    "# æ¨¡åž‹è¨“ç·´\n",
    "# =======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMPredictor(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top3 = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    print(f\"\\nðŸš€ Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)  # (batch_size, num_classes)\n",
    "\n",
    "        # è¨ˆç®—æå¤±\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # -------- æº–ç¢ºçŽ‡è¨ˆç®— --------\n",
    "        _, pred_top1 = torch.max(outputs, dim=1)\n",
    "        correct_top1 += (pred_top1 == y_batch).sum().item()\n",
    "\n",
    "        top3 = torch.topk(outputs, 3, dim=1).indices\n",
    "        for i in range(len(y_batch)):\n",
    "            if y_batch[i] in top3[i]:\n",
    "                correct_top3 += 1\n",
    "\n",
    "        total_samples += y_batch.size(0)\n",
    "        # ---------------------------\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    top1_acc = correct_top1 / total_samples\n",
    "    top3_acc = correct_top3 / total_samples\n",
    "\n",
    "    print(f\"âœ… Epoch {epoch+1} - Loss: {avg_loss:.4f} | Top-1 Accuracy: {top1_acc:.2%} | Top-3 Accuracy: {top3_acc:.2%}\")\n",
    "# =======================\n",
    "# Top-3 é æ¸¬èˆ‡è©•ä¼°\n",
    "# =======================\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        logits = model(X_batch)\n",
    "        top3 = torch.topk(logits, 3, dim=1).indices.cpu().numpy()\n",
    "        all_preds.extend(top3)\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "# Top-3 å‘½ä¸­çŽ‡\n",
    "hits = [label in pred for label, pred in zip(all_labels, all_preds)]\n",
    "top3_acc = np.mean(hits)\n",
    "print(f\"\\nâœ… Top-3 Accuracy: {top3_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afdcdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m202.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  lightgbm\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74baa4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning LogisticRegression ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning RandomForest ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      ">> Tuning HistGradientBoost ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      ">> Tuning ExtraTrees ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning LightGBM ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007740 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 10049, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093169\n",
      "[LightGBM] [Info] Start training from score -2.069244\n",
      "[LightGBM] [Info] Start training from score -2.107803\n",
      "[LightGBM] [Info] Start training from score -2.053606\n",
      "[LightGBM] [Info] Start training from score -2.082731\n",
      "[LightGBM] [Info] Start training from score -2.060613\n",
      "[LightGBM] [Info] Start training from score -2.064527\n",
      "[LightGBM] [Info] Start training from score -2.105349\n",
      "\n",
      ">> Tuning XGBoost ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Comparison ===\n",
      "             Model                                                                                                  BestParams Val_Recall@3 Test_Acc Test_Recall@3\n",
      "LogisticRegression                                               {'C': 2.1333911067827613, 'penalty': 'l2', 'solver': 'lbfgs'}       39.54%   14.57%        39.55%\n",
      "      RandomForest                                                 {'max_depth': 6, 'max_features': None, 'n_estimators': 199}       51.65%   20.78%        53.84%\n",
      " HistGradientBoost                              {'learning_rate': 0.010233629752304298, 'max_iter': 237, 'max_leaf_nodes': 30}       49.05%   19.91%        50.71%\n",
      "        ExtraTrees                                                 {'max_depth': 6, 'max_features': None, 'n_estimators': 199}       52.86%   20.86%        53.64%\n",
      "          LightGBM                              {'learning_rate': 0.010233629752304298, 'n_estimators': 237, 'num_leaves': 30}       49.36%   18.49%        50.91%\n",
      "           XGBoost {'learning_rate': 0.05958008171890075, 'max_depth': 9, 'n_estimators': 58, 'subsample': 0.8861223846483287}       48.10%   17.97%        50.00%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "# -----------------------------\n",
    "# 1. è®€æª” + æ™‚é–“æŽ’åº\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\n",
    "    \"retail_store_sales_cleaned_feature_engineering.csv\",\n",
    "    parse_dates=[\"Transaction Date\"]\n",
    ")\n",
    "df = df.sort_values([\"Customer ID\", \"Transaction Date\"])\\\n",
    "       .reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. ç›®æ¨™ç·¨ç¢¼\n",
    "# -----------------------------\n",
    "le = LabelEncoder()\n",
    "df[\"Category_ID\"] = le.fit_transform(df[\"Category\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 3. è¡ç”Ÿç‰¹å¾µï¼šInter_Daysã€30å¤©RFMã€é€±æœŸç·¨ç¢¼ã€flags\n",
    "# -----------------------------\n",
    "df[\"Prev_Date\"] = df.groupby(\"Customer ID\")[\"Transaction Date\"].shift(1)\n",
    "df[\"Inter_Days\"] = (df[\"Transaction Date\"] - df[\"Prev_Date\"])\\\n",
    "                   .dt.days.fillna(0)\n",
    "\n",
    "window_times, window_amounts = deque(), deque()\n",
    "freq30 = np.zeros(len(df), int)\n",
    "amt30_sum = np.zeros(len(df))\n",
    "amt30_count = np.zeros(len(df))\n",
    "\n",
    "for i, (cust, t, amt) in enumerate(zip(\n",
    "    df[\"Customer ID\"],\n",
    "    df[\"Transaction Date\"],\n",
    "    df[\"Total Spent\"]\n",
    ")):\n",
    "    if i == 0 or cust != df.loc[i-1, \"Customer ID\"]:\n",
    "        window_times.clear(); window_amounts.clear()\n",
    "    window_times.append(t); window_amounts.append(amt)\n",
    "    while (t - window_times[0]).days > 30:\n",
    "        window_times.popleft(); window_amounts.popleft()\n",
    "    freq30[i]     = len(window_times) - 1\n",
    "    amt30_sum[i]  = sum(window_amounts)\n",
    "    amt30_count[i]= len(window_times)\n",
    "\n",
    "df[\"freq30\"]     = freq30\n",
    "df[\"amt30_mean\"] = amt30_sum / amt30_count\n",
    "\n",
    "# é€±æœŸç·¨ç¢¼\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"day_sin\"]   = np.sin(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "df[\"day_cos\"]   = np.cos(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "\n",
    "bool_cols = [\n",
    "    \"Is_Weekend\",\"Is_Holiday\",\"Is_NonWorkday\",\n",
    "    \"PM_Credit Card\",\"PM_Digital Wallet\",\n",
    "    \"Loc_Online\",\"Disc_True\",\"Disc_Unknown\"\n",
    "]\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Price Per Unit\",\"Quantity\",\"Total Spent\",\"Recency_Cust\",\n",
    "    \"Inter_Days\",\"freq30\",\"amt30_mean\",\n",
    "    \"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"\n",
    "] + bool_cols\n",
    "\n",
    "# -----------------------------\n",
    "# 4. per-user æ™‚é–“åˆ‡åˆ† 80%/20%\n",
    "# -----------------------------\n",
    "train_list, test_list = [], []\n",
    "for _, grp in df.groupby(\"Customer ID\"):\n",
    "    grp = grp.sort_values(\"Transaction Date\").reset_index(drop=True)\n",
    "    cut = int(len(grp) * 0.8)\n",
    "    train_list.append(grp.iloc[:cut])\n",
    "    test_list.append(grp.iloc[cut:])\n",
    "\n",
    "train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "test_df  = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. æ¨™æº–åŒ–\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df [feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[\"Category_ID\"]\n",
    "X_test,  y_test  = test_df [feature_cols], test_df [\"Category_ID\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Recall@3 æ‰“åˆ†å‡½å¼ï¼ˆå…ˆè½‰æˆ numpyï¼‰\n",
    "# -----------------------------\n",
    "def recall_at_3(y_true, y_score):\n",
    "    y_true = np.array(y_true)  # <- é€™è¡Œå¾ˆé‡è¦\n",
    "    top3 = np.argsort(y_score, axis=1)[:, -3:]\n",
    "    return np.mean([y_true[i] in top3[i] for i in range(len(y_true))])\n",
    "\n",
    "recall3_scorer = make_scorer(recall_at_3, needs_proba=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. æ¨¡åž‹èˆ‡è¶…åƒæœå°‹ç©ºé–“\n",
    "# -----------------------------\n",
    "models_and_spaces = {\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(random_state=42, max_iter=1000),\n",
    "        {\n",
    "            \"C\": uniform(0.01, 10),\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"solver\": [\"lbfgs\"]\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"max_depth\": randint(3, 20),\n",
    "            \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "        }\n",
    "    ),\n",
    "    \"HistGradientBoost\": (\n",
    "        HistGradientBoostingClassifier(random_state=42),\n",
    "        {\n",
    "            \"learning_rate\": uniform(0.01, 0.3),\n",
    "            \"max_iter\": randint(50, 300),\n",
    "            \"max_leaf_nodes\": randint(10, 100)\n",
    "        }\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"max_depth\": randint(3, 20),\n",
    "            \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"learning_rate\": uniform(0.01, 0.3),\n",
    "            \"num_leaves\": randint(10, 150)\n",
    "        }\n",
    "    ),\n",
    "    # \"XGBoost\": (\n",
    "    #     XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "    #     {\n",
    "    #         \"n_estimators\": randint(50, 300),\n",
    "    #         \"learning_rate\": uniform(0.01, 0.3),\n",
    "    #         \"max_depth\": randint(3, 20),\n",
    "    #         \"subsample\": uniform(0.5, 0.5)\n",
    "    #     }\n",
    "    # )\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 8. RandomizedSearchCV\n",
    "# -----------------------------\n",
    "best_results = []\n",
    "for name, (estimator, param_dist) in models_and_spaces.items():\n",
    "    print(f\"\\n>> Tuning {name} ...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring={\"acc\": \"accuracy\", \"rec3\": recall3_scorer},\n",
    "        refit=\"rec3\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best = search.best_estimator_\n",
    "    test_acc  = accuracy_score(y_test, best.predict(X_test))\n",
    "    test_rec3 = recall_at_3(y_test, best.predict_proba(X_test))\n",
    "\n",
    "    best_results.append({\n",
    "        \"Model\": name,\n",
    "        \"BestParams\": search.best_params_,\n",
    "        \"Val_Recall@3\": f\"{search.best_score_:.2%}\",\n",
    "        \"Test_Acc\": f\"{test_acc:.2%}\",\n",
    "        \"Test_Recall@3\": f\"{test_rec3:.2%}\"\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# 9. åˆ—å°çµæžœ\n",
    "# -----------------------------\n",
    "results_df = pd.DataFrame(best_results)\n",
    "print(\"\\n=== Final Comparison ===\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02e18bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BestParams</th>\n",
       "      <th>Val_Recall@3</th>\n",
       "      <th>Test_Acc</th>\n",
       "      <th>Test_Recall@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 2.1333911067827613, 'penalty': 'l2', 'so...</td>\n",
       "      <td>39.54%</td>\n",
       "      <td>14.57%</td>\n",
       "      <td>39.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'max_depth': 6, 'max_features': None, 'n_esti...</td>\n",
       "      <td>51.65%</td>\n",
       "      <td>20.78%</td>\n",
       "      <td>53.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HistGradientBoost</td>\n",
       "      <td>{'learning_rate': 0.010233629752304298, 'max_i...</td>\n",
       "      <td>49.05%</td>\n",
       "      <td>19.91%</td>\n",
       "      <td>50.71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'max_depth': 6, 'max_features': None, 'n_esti...</td>\n",
       "      <td>52.86%</td>\n",
       "      <td>20.86%</td>\n",
       "      <td>53.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'learning_rate': 0.010233629752304298, 'n_est...</td>\n",
       "      <td>49.36%</td>\n",
       "      <td>18.49%</td>\n",
       "      <td>50.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'learning_rate': 0.05958008171890075, 'max_de...</td>\n",
       "      <td>48.10%</td>\n",
       "      <td>17.97%</td>\n",
       "      <td>50.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model                                         BestParams  \\\n",
       "0  LogisticRegression  {'C': 2.1333911067827613, 'penalty': 'l2', 'so...   \n",
       "1        RandomForest  {'max_depth': 6, 'max_features': None, 'n_esti...   \n",
       "2   HistGradientBoost  {'learning_rate': 0.010233629752304298, 'max_i...   \n",
       "3          ExtraTrees  {'max_depth': 6, 'max_features': None, 'n_esti...   \n",
       "4            LightGBM  {'learning_rate': 0.010233629752304298, 'n_est...   \n",
       "5             XGBoost  {'learning_rate': 0.05958008171890075, 'max_de...   \n",
       "\n",
       "  Val_Recall@3 Test_Acc Test_Recall@3  \n",
       "0       39.54%   14.57%        39.55%  \n",
       "1       51.65%   20.78%        53.84%  \n",
       "2       49.05%   19.91%        50.71%  \n",
       "3       52.86%   20.86%        53.64%  \n",
       "4       49.36%   18.49%        50.91%  \n",
       "5       48.10%   17.97%        50.00%  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9358bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning LogisticRegression ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning RandomForest ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      ">> Tuning HistGradientBoost ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      ">> Tuning ExtraTrees ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning LightGBM ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Start training from score -2.069194[LightGBM] [Info] Total Bins 1296\n",
      "\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.082282[LightGBM] [Info] Start training from score -2.069194\n",
      "\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.105299[LightGBM] [Info] Start training from score -2.083480\n",
      "\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1297\n",
      "[LightGBM] [Info] Number of data points in the train set: 10049, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093169\n",
      "[LightGBM] [Info] Start training from score -2.069244\n",
      "[LightGBM] [Info] Start training from score -2.107803\n",
      "[LightGBM] [Info] Start training from score -2.053606\n",
      "[LightGBM] [Info] Start training from score -2.082731\n",
      "[LightGBM] [Info] Start training from score -2.060613\n",
      "[LightGBM] [Info] Start training from score -2.064527\n",
      "[LightGBM] [Info] Start training from score -2.105349\n",
      "\n",
      "=== Final Comparison ===\n",
      "             Model                                                                     BestParams Val_Recall@3 Test_Acc Test_Recall@3\n",
      "LogisticRegression                 {'C': 0.21584494295802448, 'penalty': 'l2', 'solver': 'lbfgs'}       39.89%   13.66%        40.02%\n",
      "      RandomForest                    {'max_depth': 6, 'max_features': None, 'n_estimators': 199}       51.17%   20.55%        53.52%\n",
      " HistGradientBoost {'learning_rate': 0.010233629752304298, 'max_iter': 237, 'max_leaf_nodes': 30}       48.19%   18.69%        50.36%\n",
      "        ExtraTrees                    {'max_depth': 6, 'max_features': None, 'n_estimators': 199}       52.37%   20.59%        53.92%\n",
      "          LightGBM {'learning_rate': 0.010233629752304298, 'n_estimators': 237, 'num_leaves': 30}       48.24%   18.05%        50.36%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "# -----------------------------\n",
    "# 1. è®€æª” + æ™‚é–“æŽ’åº\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\n",
    "    \"retail_store_sales_cleaned_feature_engineering.csv\",\n",
    "    parse_dates=[\"Transaction Date\"]\n",
    ")\n",
    "df = df.sort_values([\"Customer ID\", \"Transaction Date\"])\\\n",
    "       .reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. ç›®æ¨™ç·¨ç¢¼\n",
    "# -----------------------------\n",
    "le = LabelEncoder()\n",
    "df[\"Category_ID\"] = le.fit_transform(df[\"Category\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 3. è¡ç”Ÿç‰¹å¾µï¼šInter_Daysã€30å¤©RFMã€é€±æœŸç·¨ç¢¼ã€flags\n",
    "# -----------------------------\n",
    "df[\"Prev_Date\"] = df.groupby(\"Customer ID\")[\"Transaction Date\"].shift(1)\n",
    "df[\"Inter_Days\"] = (df[\"Transaction Date\"] - df[\"Prev_Date\"])\\\n",
    "                   .dt.days.fillna(0)\n",
    "\n",
    "window_times, window_amounts = deque(), deque()\n",
    "freq30 = np.zeros(len(df), int)\n",
    "amt30_sum = np.zeros(len(df))\n",
    "amt30_count = np.zeros(len(df))\n",
    "\n",
    "for i, (cust, t, amt) in enumerate(zip(\n",
    "    df[\"Customer ID\"],\n",
    "    df[\"Transaction Date\"],\n",
    "    df[\"Total Spent\"]\n",
    ")):\n",
    "    if i == 0 or cust != df.loc[i-1, \"Customer ID\"]:\n",
    "        window_times.clear(); window_amounts.clear()\n",
    "    window_times.append(t); window_amounts.append(amt)\n",
    "    while (t - window_times[0]).days > 30:\n",
    "        window_times.popleft(); window_amounts.popleft()\n",
    "    freq30[i]     = len(window_times) - 1\n",
    "    amt30_sum[i]  = sum(window_amounts)\n",
    "    amt30_count[i]= len(window_times)\n",
    "\n",
    "df[\"freq30\"]     = freq30\n",
    "df[\"amt30_mean\"] = amt30_sum / amt30_count\n",
    "\n",
    "# é€±æœŸç·¨ç¢¼\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"day_sin\"]   = np.sin(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "df[\"day_cos\"]   = np.cos(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "\n",
    "bool_cols = [\n",
    "    \"Is_Weekend\",\"Is_Holiday\",\"Is_NonWorkday\",\n",
    "    \"PM_Credit Card\",\"PM_Digital Wallet\",\n",
    "    \"Loc_Online\",\"Disc_True\",\"Disc_Unknown\"\n",
    "]\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Price Per Unit\",\"Quantity\",\"Total Spent\",\"Recency_Cust\",\n",
    "    \"Inter_Days\",\"freq30\",\"amt30_mean\",\n",
    "    \"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"\n",
    "] + bool_cols\n",
    "\n",
    "def rolling_rfm(df, window_days, amt_col=\"Total Spent\"):\n",
    "    times, amts, freqs, sums = deque(), deque(), [], []\n",
    "    for cust, group in df.groupby(\"Customer ID\", sort=False):\n",
    "        times.clear(); amts.clear()\n",
    "        for t, amt in zip(group[\"Transaction Date\"], group[amt_col]):\n",
    "            times.append(t); amts.append(amt)\n",
    "            # pop è¶…å‡º window_days\n",
    "            while (t - times[0]).days > window_days:\n",
    "                times.popleft(); amts.popleft()\n",
    "            freqs.append(len(times)-1)            # æ‰£æŽ‰è‡ªå·±\n",
    "            sums.append(sum(amts))\n",
    "    return freqs, sums\n",
    "\n",
    "df = df.sort_values([\"Customer ID\",\"Transaction Date\"]).reset_index(drop=True)\n",
    "# 7 å¤©\n",
    "df[\"freq7\"],  df[\"amt7_sum\"]  = rolling_rfm(df,  7)\n",
    "# 30 å¤©ï¼ˆä»¥å‰å·²æœ‰ freq30, amt30_meanï¼‰ï¼Œå¦‚æžœæƒ³ä¿ç•™ sum å¯è¦†è“‹æˆ–æ”¹å\n",
    "# 90 å¤©\n",
    "df[\"freq90\"], df[\"amt90_sum\"] = rolling_rfm(df, 90)\n",
    "\n",
    "# ç‚ºäº†è·Ÿ amt30_mean åŒæ­¥ï¼Œä¸Šé¢ sum æ”¹æˆ mean ä¹Ÿå¾ˆç°¡å–®ï¼š\n",
    "df[\"amt7_mean\"]  = df[\"amt7_sum\"]  / (df[\"freq7\"]  + 1)  # +1 åŒ…å«ç•¶å‰äº¤æ˜“\n",
    "df[\"amt90_mean\"] = df[\"amt90_sum\"] / (df[\"freq90\"] + 1)\n",
    "\n",
    "# --- 2. è·ä»Šæœ€å¾Œè³¼è²·æ—¥çš„æŒ‡æ•¸è¡°æ¸›ç‰¹å¾µï¼ˆRecencyï¼‰ ---------------------\n",
    "\n",
    "# å…ˆç®—ã€Œè·ä»Šå¤©æ•¸ã€\n",
    "today = df[\"Transaction Date\"].max()  # or datetime.today()\n",
    "last_purchase = df.groupby(\"Customer ID\")[\"Transaction Date\"].transform(\"max\")\n",
    "df[\"days_since_last\"] = (today - last_purchase).dt.days\n",
    "\n",
    "# å†åšæŒ‡æ•¸è¡°æ¸›ï¼šexp(-Î»Â·days)ï¼ŒÎ» å¯èª¿\n",
    "lam = 0.05\n",
    "df[\"recency_exp\"] = np.exp(-lam * df[\"days_since_last\"])\n",
    "\n",
    "# --- 3. å®¢æˆ¶æ•´é«” RFM èšé¡ž -----------------------------------------\n",
    "\n",
    "# ç‚ºæ¯ä½å®¢æˆ¶è¨ˆç®—æ•´é«” RFM å‘é‡\n",
    "rfm = df.groupby(\"Customer ID\").agg({\n",
    "    \"days_since_last\": \"min\",               # Recencyï¼šæœ€è¿‘é‚£ç­†äº¤æ˜“è·ä»Š\n",
    "    \"Transaction ID\": \"count\",              # Frequencyï¼šäº¤æ˜“æ¬¡æ•¸\n",
    "    \"Total Spent\": \"sum\"                    # Monetaryï¼šç¸½èŠ±è²»\n",
    "}).rename(columns={\n",
    "    \"days_since_last\": \"R\",\n",
    "    \"Transaction ID\": \"F\",\n",
    "    \"Total Spent\": \"M\"\n",
    "})\n",
    "\n",
    "# æ¨™æº–åŒ–å¾Œåš KMeans\n",
    "scaler_rfm = StandardScaler()\n",
    "rfm_scaled = scaler_rfm.fit_transform(rfm[[\"R\",\"F\",\"M\"]])\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "rfm[\"cluster\"] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Merge å›žåŽŸå§‹ df\n",
    "df = df.merge(rfm[\"cluster\"], left_on=\"Customer ID\", right_index=True)\n",
    "\n",
    "# --- 4. æ›´æ–° feature_cols ---------------------------------------\n",
    "\n",
    "new_feats = [\n",
    "    \"freq7\", \"amt7_mean\",\n",
    "    \"freq90\", \"amt90_mean\",\n",
    "    \"days_since_last\", \"recency_exp\",\n",
    "    \"cluster\"\n",
    "]\n",
    "feature_cols += new_feats\n",
    "\n",
    "\n",
    "train_list, test_list = [], []\n",
    "for _, grp in df.groupby(\"Customer ID\"):\n",
    "    grp = grp.sort_values(\"Transaction Date\").reset_index(drop=True)\n",
    "    cut = int(len(grp) * 0.8)\n",
    "    train_list.append(grp.iloc[:cut])\n",
    "    test_list.append(grp.iloc[cut:])\n",
    "\n",
    "train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "test_df  = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. æ¨™æº–åŒ–\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df [feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[\"Category_ID\"]\n",
    "X_test,  y_test  = test_df [feature_cols], test_df [\"Category_ID\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Recall@3 æ‰“åˆ†å‡½å¼ï¼ˆå…ˆè½‰æˆ numpyï¼‰\n",
    "# -----------------------------\n",
    "def recall_at_3(y_true, y_score):\n",
    "    y_true = np.array(y_true)  # <- é€™è¡Œå¾ˆé‡è¦\n",
    "    top3 = np.argsort(y_score, axis=1)[:, -3:]\n",
    "    return np.mean([y_true[i] in top3[i] for i in range(len(y_true))])\n",
    "\n",
    "recall3_scorer = make_scorer(recall_at_3, needs_proba=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. æ¨¡åž‹èˆ‡è¶…åƒæœå°‹ç©ºé–“\n",
    "# -----------------------------\n",
    "models_and_spaces = {\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(random_state=42, max_iter=1000),\n",
    "        {\n",
    "            \"C\": uniform(0.01, 10),\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"solver\": [\"lbfgs\"]\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"max_depth\": randint(3, 20),\n",
    "            \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "        }\n",
    "    ),\n",
    "    \"HistGradientBoost\": (\n",
    "        HistGradientBoostingClassifier(random_state=42),\n",
    "        {\n",
    "            \"learning_rate\": uniform(0.01, 0.3),\n",
    "            \"max_iter\": randint(50, 300),\n",
    "            \"max_leaf_nodes\": randint(10, 100)\n",
    "        }\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"max_depth\": randint(3, 20),\n",
    "            \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"learning_rate\": uniform(0.01, 0.3),\n",
    "            \"num_leaves\": randint(10, 150)\n",
    "        }\n",
    "    ),\n",
    "    # \"XGBoost\": (\n",
    "    #     XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "    #     {\n",
    "    #         \"n_estimators\": randint(50, 300),\n",
    "    #         \"learning_rate\": uniform(0.01, 0.3),\n",
    "    #         \"max_depth\": randint(3, 20),\n",
    "    #         \"subsample\": uniform(0.5, 0.5)\n",
    "    #     }\n",
    "    # )\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 8. RandomizedSearchCV\n",
    "# -----------------------------\n",
    "best_results = []\n",
    "for name, (estimator, param_dist) in models_and_spaces.items():\n",
    "    print(f\"\\n>> Tuning {name} ...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring={\"acc\": \"accuracy\", \"rec3\": recall3_scorer},\n",
    "        refit=\"rec3\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best = search.best_estimator_\n",
    "    test_acc  = accuracy_score(y_test, best.predict(X_test))\n",
    "    test_rec3 = recall_at_3(y_test, best.predict_proba(X_test))\n",
    "\n",
    "    best_results.append({\n",
    "        \"Model\": name,\n",
    "        \"BestParams\": search.best_params_,\n",
    "        \"Val_Recall@3\": f\"{search.best_score_:.2%}\",\n",
    "        \"Test_Acc\": f\"{test_acc:.2%}\",\n",
    "        \"Test_Recall@3\": f\"{test_rec3:.2%}\"\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# 9. åˆ—å°çµæžœ\n",
    "# -----------------------------\n",
    "results_df = pd.DataFrame(best_results)\n",
    "print(\"\\n=== Final Comparison ===\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f460a7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BestParams</th>\n",
       "      <th>Val_Recall@3</th>\n",
       "      <th>Test_Acc</th>\n",
       "      <th>Test_Recall@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 0.21584494295802448, 'penalty': 'l2', 's...</td>\n",
       "      <td>39.89%</td>\n",
       "      <td>13.66%</td>\n",
       "      <td>40.02%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'max_depth': 6, 'max_features': None, 'n_esti...</td>\n",
       "      <td>51.17%</td>\n",
       "      <td>20.55%</td>\n",
       "      <td>53.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HistGradientBoost</td>\n",
       "      <td>{'learning_rate': 0.010233629752304298, 'max_i...</td>\n",
       "      <td>48.19%</td>\n",
       "      <td>18.69%</td>\n",
       "      <td>50.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'max_depth': 6, 'max_features': None, 'n_esti...</td>\n",
       "      <td>52.37%</td>\n",
       "      <td>20.59%</td>\n",
       "      <td>53.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'learning_rate': 0.010233629752304298, 'n_est...</td>\n",
       "      <td>48.24%</td>\n",
       "      <td>18.05%</td>\n",
       "      <td>50.36%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model                                         BestParams  \\\n",
       "0  LogisticRegression  {'C': 0.21584494295802448, 'penalty': 'l2', 's...   \n",
       "1        RandomForest  {'max_depth': 6, 'max_features': None, 'n_esti...   \n",
       "2   HistGradientBoost  {'learning_rate': 0.010233629752304298, 'max_i...   \n",
       "3          ExtraTrees  {'max_depth': 6, 'max_features': None, 'n_esti...   \n",
       "4            LightGBM  {'learning_rate': 0.010233629752304298, 'n_est...   \n",
       "\n",
       "  Val_Recall@3 Test_Acc Test_Recall@3  \n",
       "0       39.89%   13.66%        40.02%  \n",
       "1       51.17%   20.55%        53.52%  \n",
       "2       48.19%   18.69%        50.36%  \n",
       "3       52.37%   20.59%        53.92%  \n",
       "4       48.24%   18.05%        50.36%  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca4175e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1303\n",
      "[LightGBM] [Info] Number of data points in the train set: 40004, number of used features: 34\n",
      "[10]\tvalid_0's ndcg@3: 0.106144\n",
      "[20]\tvalid_0's ndcg@3: 0\n",
      "[30]\tvalid_0's ndcg@3: 0.0469279\n",
      "[40]\tvalid_0's ndcg@3: 0\n",
      "[50]\tvalid_0's ndcg@3: 0\n",
      "[60]\tvalid_0's ndcg@3: 0.0592164\n",
      "[70]\tvalid_0's ndcg@3: 0.0592164\n",
      "[80]\tvalid_0's ndcg@3: 0\n",
      "[90]\tvalid_0's ndcg@3: 0\n",
      "[100]\tvalid_0's ndcg@3: 0.0469279\n",
      "[110]\tvalid_0's ndcg@3: 0.0469279\n",
      "[120]\tvalid_0's ndcg@3: 0\n",
      "[130]\tvalid_0's ndcg@3: 0.0592164\n",
      "[140]\tvalid_0's ndcg@3: 0.106144\n",
      "[150]\tvalid_0's ndcg@3: 0.106144\n",
      "[160]\tvalid_0's ndcg@3: 0.106144\n",
      "[170]\tvalid_0's ndcg@3: 0.106144\n",
      "[180]\tvalid_0's ndcg@3: 0.0592164\n",
      "[190]\tvalid_0's ndcg@3: 0.106144\n",
      "[200]\tvalid_0's ndcg@3: 0.153072\n",
      "[210]\tvalid_0's ndcg@3: 0.153072\n",
      "[220]\tvalid_0's ndcg@3: 0.2\n",
      "[230]\tvalid_0's ndcg@3: 0.224577\n",
      "[240]\tvalid_0's ndcg@3: 0.259216\n",
      "[250]\tvalid_0's ndcg@3: 0.259216\n",
      "[260]\tvalid_0's ndcg@3: 0.259216\n",
      "[270]\tvalid_0's ndcg@3: 0.259216\n",
      "[280]\tvalid_0's ndcg@3: 0.259216\n",
      "[290]\tvalid_0's ndcg@3: 0.224577\n",
      "[300]\tvalid_0's ndcg@3: 0.271505\n",
      "[310]\tvalid_0's ndcg@3: 0.224577\n",
      "[320]\tvalid_0's ndcg@3: 0.224577\n",
      "[330]\tvalid_0's ndcg@3: 0.212289\n",
      "[340]\tvalid_0's ndcg@3: 0.224577\n",
      "[350]\tvalid_0's ndcg@3: 0.259216\n",
      "[360]\tvalid_0's ndcg@3: 0.246928\n",
      "[370]\tvalid_0's ndcg@3: 0.259216\n",
      "[380]\tvalid_0's ndcg@3: 0.259216\n",
      "[390]\tvalid_0's ndcg@3: 0.246928\n",
      "[400]\tvalid_0's ndcg@3: 0.246928\n",
      "[410]\tvalid_0's ndcg@3: 0.246928\n",
      "[420]\tvalid_0's ndcg@3: 0.246928\n",
      "[430]\tvalid_0's ndcg@3: 0.246928\n",
      "[440]\tvalid_0's ndcg@3: 0.246928\n",
      "[450]\tvalid_0's ndcg@3: 0.246928\n",
      "[460]\tvalid_0's ndcg@3: 0.246928\n",
      "[470]\tvalid_0's ndcg@3: 0.246928\n",
      "[480]\tvalid_0's ndcg@3: 0.306144\n",
      "[490]\tvalid_0's ndcg@3: 0.306144\n",
      "[500]\tvalid_0's ndcg@3: 0.306144\n",
      "[510]\tvalid_0's ndcg@3: 0.306144\n",
      "[520]\tvalid_0's ndcg@3: 0.306144\n",
      "[530]\tvalid_0's ndcg@3: 0.306144\n",
      "[540]\tvalid_0's ndcg@3: 0.340784\n",
      "[550]\tvalid_0's ndcg@3: 0.340784\n",
      "[560]\tvalid_0's ndcg@3: 0.306144\n",
      "[570]\tvalid_0's ndcg@3: 0.306144\n",
      "[580]\tvalid_0's ndcg@3: 0.306144\n",
      "[590]\tvalid_0's ndcg@3: 0.306144\n",
      "[600]\tvalid_0's ndcg@3: 0.306144\n",
      "[610]\tvalid_0's ndcg@3: 0.306144\n",
      "[620]\tvalid_0's ndcg@3: 0.306144\n",
      "[630]\tvalid_0's ndcg@3: 0.306144\n",
      "[640]\tvalid_0's ndcg@3: 0.306144\n",
      "[650]\tvalid_0's ndcg@3: 0.306144\n",
      "[660]\tvalid_0's ndcg@3: 0.306144\n",
      "[670]\tvalid_0's ndcg@3: 0.306144\n",
      "[680]\tvalid_0's ndcg@3: 0.306144\n",
      "[690]\tvalid_0's ndcg@3: 0.306144\n",
      "[700]\tvalid_0's ndcg@3: 0.306144\n",
      "[710]\tvalid_0's ndcg@3: 0.306144\n",
      "[720]\tvalid_0's ndcg@3: 0.306144\n",
      "[730]\tvalid_0's ndcg@3: 0.306144\n",
      "[740]\tvalid_0's ndcg@3: 0.353072\n",
      "[750]\tvalid_0's ndcg@3: 0.4\n",
      "[760]\tvalid_0's ndcg@3: 0.4\n",
      "[770]\tvalid_0's ndcg@3: 0.4\n",
      "[780]\tvalid_0's ndcg@3: 0.4\n",
      "[790]\tvalid_0's ndcg@3: 0.4\n",
      "[800]\tvalid_0's ndcg@3: 0.4\n",
      "[810]\tvalid_0's ndcg@3: 0.4\n",
      "[820]\tvalid_0's ndcg@3: 0.446928\n",
      "[830]\tvalid_0's ndcg@3: 0.446928\n",
      "[840]\tvalid_0's ndcg@3: 0.481567\n",
      "[850]\tvalid_0's ndcg@3: 0.481567\n",
      "[860]\tvalid_0's ndcg@3: 0.481567\n",
      "[870]\tvalid_0's ndcg@3: 0.481567\n",
      "[880]\tvalid_0's ndcg@3: 0.481567\n",
      "[890]\tvalid_0's ndcg@3: 0.481567\n",
      "[900]\tvalid_0's ndcg@3: 0.481567\n",
      "[910]\tvalid_0's ndcg@3: 0.481567\n",
      "[920]\tvalid_0's ndcg@3: 0.481567\n",
      "[930]\tvalid_0's ndcg@3: 0.434639\n",
      "[940]\tvalid_0's ndcg@3: 0.434639\n",
      "[950]\tvalid_0's ndcg@3: 0.434639\n",
      "[960]\tvalid_0's ndcg@3: 0.481567\n",
      "[970]\tvalid_0's ndcg@3: 0.481567\n",
      "[980]\tvalid_0's ndcg@3: 0.481567\n",
      "[990]\tvalid_0's ndcg@3: 0.481567\n",
      "[1000]\tvalid_0's ndcg@3: 0.481567\n",
      "Ranker Test Top-3 Recall:  0.8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.metrics import ndcg_score, recall_score\n",
    "from lightgbm import LGBMRanker, log_evaluation, early_stopping\n",
    "# 1. è¯»å–å¹¶é¢„å¤„ç†ï¼ˆè·Ÿä½ ä¹‹å‰çš„ pipeline ä¸€è‡´ï¼Œåªç•™å…³é”®æ­¥éª¤ï¼‰\n",
    "df = pd.read_csv(\"retail_store_sales_cleaned_feature_engineering.csv\",\n",
    "                 parse_dates=[\"Transaction Date\"])\n",
    "df = df.sort_values([\"Customer ID\", \"Transaction Date\"]).reset_index(drop=True)\n",
    "\n",
    "# å‡è®¾ä½ å·²ç»åšå®Œæ‰€æœ‰ç‰¹å¾å·¥ç¨‹ï¼Œå¹¶æŠŠ RFMã€å¤šçª—å£ç‰¹å¾ã€èšç±»ç‰¹å¾éƒ½åŠ è¿›æ¥äº†ï¼š\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"Category_ID\"] = le.fit_transform(df[\"Category\"])\n",
    "# -----------------------------\n",
    "# 3. è¡ç”Ÿç‰¹å¾µï¼šInter_Daysã€30å¤©RFMã€é€±æœŸç·¨ç¢¼ã€flags\n",
    "# -----------------------------\n",
    "df[\"Prev_Date\"] = df.groupby(\"Customer ID\")[\"Transaction Date\"].shift(1)\n",
    "df[\"Inter_Days\"] = (df[\"Transaction Date\"] - df[\"Prev_Date\"])\\\n",
    "                   .dt.days.fillna(0)\n",
    "\n",
    "window_times, window_amounts = deque(), deque()\n",
    "freq30 = np.zeros(len(df), int)\n",
    "amt30_sum = np.zeros(len(df))\n",
    "amt30_count = np.zeros(len(df))\n",
    "\n",
    "for i, (cust, t, amt) in enumerate(zip(\n",
    "    df[\"Customer ID\"],\n",
    "    df[\"Transaction Date\"],\n",
    "    df[\"Total Spent\"]\n",
    ")):\n",
    "    if i == 0 or cust != df.loc[i-1, \"Customer ID\"]:\n",
    "        window_times.clear(); window_amounts.clear()\n",
    "    window_times.append(t); window_amounts.append(amt)\n",
    "    while (t - window_times[0]).days > 30:\n",
    "        window_times.popleft(); window_amounts.popleft()\n",
    "    freq30[i]     = len(window_times) - 1\n",
    "    amt30_sum[i]  = sum(window_amounts)\n",
    "    amt30_count[i]= len(window_times)\n",
    "\n",
    "df[\"freq30\"]     = freq30\n",
    "df[\"amt30_mean\"] = amt30_sum / amt30_count\n",
    "\n",
    "# é€±æœŸç·¨ç¢¼\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"day_sin\"]   = np.sin(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "df[\"day_cos\"]   = np.cos(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "\n",
    "bool_cols = [\n",
    "    \"Is_Weekend\",\"Is_Holiday\",\"Is_NonWorkday\",\n",
    "    \"PM_Credit Card\",\"PM_Digital Wallet\",\n",
    "    \"Loc_Online\",\"Disc_True\",\"Disc_Unknown\"\n",
    "]\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Price Per Unit\",\"Quantity\",\"Total Spent\",\"Recency_Cust\",\n",
    "    \"Inter_Days\",\"freq30\",\"amt30_mean\",\n",
    "    \"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"\n",
    "] + bool_cols\n",
    "\n",
    "def rolling_rfm(df, window_days, amt_col=\"Total Spent\"):\n",
    "    times, amts, freqs, sums = deque(), deque(), [], []\n",
    "    for cust, group in df.groupby(\"Customer ID\", sort=False):\n",
    "        times.clear(); amts.clear()\n",
    "        for t, amt in zip(group[\"Transaction Date\"], group[amt_col]):\n",
    "            times.append(t); amts.append(amt)\n",
    "            # pop è¶…å‡º window_days\n",
    "            while (t - times[0]).days > window_days:\n",
    "                times.popleft(); amts.popleft()\n",
    "            freqs.append(len(times)-1)            # æ‰£æŽ‰è‡ªå·±\n",
    "            sums.append(sum(amts))\n",
    "    return freqs, sums\n",
    "\n",
    "df = df.sort_values([\"Customer ID\",\"Transaction Date\"]).reset_index(drop=True)\n",
    "# 7 å¤©\n",
    "df[\"freq7\"],  df[\"amt7_sum\"]  = rolling_rfm(df,  7)\n",
    "# 30 å¤©ï¼ˆä»¥å‰å·²æœ‰ freq30, amt30_meanï¼‰ï¼Œå¦‚æžœæƒ³ä¿ç•™ sum å¯è¦†è“‹æˆ–æ”¹å\n",
    "# 90 å¤©\n",
    "df[\"freq90\"], df[\"amt90_sum\"] = rolling_rfm(df, 90)\n",
    "\n",
    "# ç‚ºäº†è·Ÿ amt30_mean åŒæ­¥ï¼Œä¸Šé¢ sum æ”¹æˆ mean ä¹Ÿå¾ˆç°¡å–®ï¼š\n",
    "df[\"amt7_mean\"]  = df[\"amt7_sum\"]  / (df[\"freq7\"]  + 1)  # +1 åŒ…å«ç•¶å‰äº¤æ˜“\n",
    "df[\"amt90_mean\"] = df[\"amt90_sum\"] / (df[\"freq90\"] + 1)\n",
    "\n",
    "# --- 2. è·ä»Šæœ€å¾Œè³¼è²·æ—¥çš„æŒ‡æ•¸è¡°æ¸›ç‰¹å¾µï¼ˆRecencyï¼‰ ---------------------\n",
    "\n",
    "# å…ˆç®—ã€Œè·ä»Šå¤©æ•¸ã€\n",
    "today = df[\"Transaction Date\"].max()  # or datetime.today()\n",
    "last_purchase = df.groupby(\"Customer ID\")[\"Transaction Date\"].transform(\"max\")\n",
    "df[\"days_since_last\"] = (today - last_purchase).dt.days\n",
    "\n",
    "# å†åšæŒ‡æ•¸è¡°æ¸›ï¼šexp(-Î»Â·days)ï¼ŒÎ» å¯èª¿\n",
    "lam = 0.05\n",
    "df[\"recency_exp\"] = np.exp(-lam * df[\"days_since_last\"])\n",
    "\n",
    "# --- 3. å®¢æˆ¶æ•´é«” RFM èšé¡ž -----------------------------------------\n",
    "\n",
    "# ç‚ºæ¯ä½å®¢æˆ¶è¨ˆç®—æ•´é«” RFM å‘é‡\n",
    "rfm = df.groupby(\"Customer ID\").agg({\n",
    "    \"days_since_last\": \"min\",               # Recencyï¼šæœ€è¿‘é‚£ç­†äº¤æ˜“è·ä»Š\n",
    "    \"Transaction ID\": \"count\",              # Frequencyï¼šäº¤æ˜“æ¬¡æ•¸\n",
    "    \"Total Spent\": \"sum\"                    # Monetaryï¼šç¸½èŠ±è²»\n",
    "}).rename(columns={\n",
    "    \"days_since_last\": \"R\",\n",
    "    \"Transaction ID\": \"F\",\n",
    "    \"Total Spent\": \"M\"\n",
    "})\n",
    "\n",
    "# æ¨™æº–åŒ–å¾Œåš KMeans\n",
    "scaler_rfm = StandardScaler()\n",
    "rfm_scaled = scaler_rfm.fit_transform(rfm[[\"R\",\"F\",\"M\"]])\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "rfm[\"cluster\"] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Merge å›žåŽŸå§‹ df\n",
    "df = df.merge(rfm[\"cluster\"], left_on=\"Customer ID\", right_index=True)\n",
    "\n",
    "# --- 4. æ›´æ–° feature_cols ---------------------------------------\n",
    "\n",
    "new_feats = [\n",
    "    \"freq7\", \"amt7_mean\",\n",
    "    \"freq90\", \"amt90_mean\",\n",
    "    \"days_since_last\", \"recency_exp\",\n",
    "    \"cluster\"\n",
    "]\n",
    "feature_cols += new_feats\n",
    "# 2. ä¸ºæ¯ä¸ªâ€œæŸ¥è¯¢â€ï¼ˆqueryï¼‰ï¼Œå³æ¯ä¸€æ¬¡è¦é¢„æµ‹ä¸‹ä¸€å“ç±»çš„åŽ†å²åºåˆ—ï¼Œæž„é€ æ­£è´Ÿæ ·æœ¬ï¼š\n",
    "#    æŠŠæ¯æ¡æ­£æ ·æœ¬ (seq â†’ true next Category_ID) å’Œéšæœºé‡‡çš„å‡ ä¸ªè´Ÿæ ·æœ¬æ‹¼åˆ°ä¸€èµ·ï¼Œç”¨åŒä¸€ä¸ª query_id åˆ†ç»„ã€‚\n",
    "records = []\n",
    "for cust, grp in df.groupby(\"Customer ID\"):\n",
    "    cats = grp[\"Category_ID\"].values\n",
    "    feats = grp[feature_cols].values\n",
    "    for i in range(1, len(grp)):\n",
    "        hist_feat = feats[i-1]  # è¿™é‡Œç”¨â€œä¸Šä¸€ç¬”â€ç‰¹å¾ï¼Œä¹Ÿå¯ä»¥ç”¨çª—å£èšåˆç­‰\n",
    "        true_cat  = cats[i]\n",
    "        # æ­£æ ·æœ¬\n",
    "        records.append((cust, hist_feat, true_cat, 1))\n",
    "        # è´Ÿé‡‡æ · 3 ä¸ª\n",
    "        for _ in range(3):\n",
    "            neg = np.random.randint(0, df[\"Category_ID\"].nunique())\n",
    "            while neg == true_cat:\n",
    "                neg = np.random.randint(0, df[\"Category_ID\"].nunique())\n",
    "            records.append((cust, hist_feat, neg, 0))\n",
    "\n",
    "rank_df = pd.DataFrame(records, columns=[\"query_id\",\"feat\",\"cat_id\",\"label\"])\n",
    "\n",
    "# 3. æž„é€ è®­ç»ƒçŸ©é˜µï¼šå°† feat å±•å¼€ã€cat_id ç”¨ one-hot æˆ– embedding ç‰¹å¾æ‹¼è¿›åŽ»\n",
    "#    è¿™é‡Œç¤ºä¾‹ï¼šç›´æŽ¥æŠŠ feat + â€œcat_idâ€ åš one-hotï¼ˆä¹Ÿå¯ç”¨ embedding lookupï¼‰ã€‚\n",
    "X = np.stack(rank_df[\"feat\"].values)\n",
    "# one-hot encode cat_idï¼š\n",
    "K = df[\"Category_ID\"].nunique()\n",
    "onehots = np.eye(K)[rank_df[\"cat_id\"].values]\n",
    "X = np.hstack([X, onehots])\n",
    "\n",
    "y = rank_df[\"label\"].values\n",
    "# group sizes = æ¯ä¸ª query_id ä¸‹çš„æ ·æœ¬æ•°é‡\n",
    "group = rank_df.groupby(\"query_id\").size().astype(int).tolist()\n",
    "\n",
    "# 4. åˆ‡åˆ†ï¼ˆä¿è¯ query æ•´ä½“ä¸æ‹†åˆ†ï¼‰\n",
    "qids = rank_df[\"query_id\"].unique()\n",
    "q_train, q_test = train_test_split(qids, test_size=0.2, random_state=42)\n",
    "train_mask = rank_df[\"query_id\"].isin(q_train)\n",
    "X_train, y_train, g_train = X[train_mask], y[train_mask], rank_df[train_mask]\\\n",
    "    .groupby(\"query_id\")\\\n",
    "    .size()\\\n",
    "    .astype(int)\\\n",
    "    .tolist()\n",
    "X_test, y_test, g_test = X[~train_mask], y[~train_mask], rank_df[~train_mask]\\\n",
    "    .groupby(\"query_id\")\\\n",
    "    .size()\\\n",
    "    .astype(int)\\\n",
    "    .tolist()\n",
    "\n",
    "# 5. ç”¨ LGBMRanker è®­ç»ƒ\n",
    "callbacks = [\n",
    "    log_evaluation(period=10),\n",
    "\n",
    "]\n",
    "\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    random_state=42,\n",
    "    importance_type=\"gain\"\n",
    ")\n",
    "\n",
    "ranker.fit(\n",
    "    X_train, y_train,\n",
    "    group=g_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_group=[g_test],\n",
    "    eval_at=[3],          # NDCG@3\n",
    "    callbacks=callbacks    # â† åœ¨è¿™é‡Œä¼ å…¥ callbacksï¼Œå–ä»£ verbose\n",
    ")\n",
    "# 6. è¯„ä¼° Top-3 Recall\n",
    "#    å¯¹æµ‹è¯•æ¯ä¸ª queryï¼Œpredict å¾—åˆ†åŽå– top3ï¼Œçœ‹æ­£æ ·æœ¬æ˜¯å¦åœ¨å†…\n",
    "pred_scores = ranker.predict(X_test)\n",
    "# é‡ç»„ä¸º per-query scores\n",
    "offset = 0\n",
    "hits = []\n",
    "for grp_size in g_test:\n",
    "    grp_scores = pred_scores[offset:offset+grp_size]\n",
    "    grp_labels = y_test      [offset:offset+grp_size]\n",
    "    topk_idx   = np.argsort(grp_scores)[-3:]\n",
    "    # åªè¦æ­£æ ·æœ¬(label=1)åœ¨ top3 å°±ç®—å‘½ä¸­\n",
    "    hits.append(any(grp_labels[i]==1 for i in topk_idx))\n",
    "    offset += grp_size\n",
    "\n",
    "print(\"Ranker Test Top-3 Recall: \", np.mean(hits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8654eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1305\n",
      "[LightGBM] [Info] Number of data points in the train set: 80320, number of used features: 34\n",
      "[10]\tvalid_0's ndcg@3: 0.274812\n",
      "[20]\tvalid_0's ndcg@3: 0.276216\n",
      "[30]\tvalid_0's ndcg@3: 0.276363\n",
      "[40]\tvalid_0's ndcg@3: 0.279313\n",
      "[50]\tvalid_0's ndcg@3: 0.272303\n",
      "[60]\tvalid_0's ndcg@3: 0.27135\n",
      "[70]\tvalid_0's ndcg@3: 0.271317\n",
      "[80]\tvalid_0's ndcg@3: 0.276088\n",
      "[90]\tvalid_0's ndcg@3: 0.272052\n",
      "[100]\tvalid_0's ndcg@3: 0.271577\n",
      "[110]\tvalid_0's ndcg@3: 0.2718\n",
      "[120]\tvalid_0's ndcg@3: 0.270657\n",
      "[130]\tvalid_0's ndcg@3: 0.268855\n",
      "[140]\tvalid_0's ndcg@3: 0.265246\n",
      "[150]\tvalid_0's ndcg@3: 0.264112\n",
      "[160]\tvalid_0's ndcg@3: 0.265046\n",
      "[170]\tvalid_0's ndcg@3: 0.266398\n",
      "[180]\tvalid_0's ndcg@3: 0.266588\n",
      "[190]\tvalid_0's ndcg@3: 0.267029\n",
      "[200]\tvalid_0's ndcg@3: 0.267982\n",
      "[210]\tvalid_0's ndcg@3: 0.269021\n",
      "[220]\tvalid_0's ndcg@3: 0.269514\n",
      "[230]\tvalid_0's ndcg@3: 0.266882\n",
      "[240]\tvalid_0's ndcg@3: 0.268286\n",
      "[250]\tvalid_0's ndcg@3: 0.268822\n",
      "[260]\tvalid_0's ndcg@3: 0.270804\n",
      "[270]\tvalid_0's ndcg@3: 0.270382\n",
      "[280]\tvalid_0's ndcg@3: 0.270781\n",
      "[290]\tvalid_0's ndcg@3: 0.269405\n",
      "[300]\tvalid_0's ndcg@3: 0.270937\n",
      "[310]\tvalid_0's ndcg@3: 0.271933\n",
      "[320]\tvalid_0's ndcg@3: 0.273138\n",
      "[330]\tvalid_0's ndcg@3: 0.273788\n",
      "[340]\tvalid_0's ndcg@3: 0.274523\n",
      "[350]\tvalid_0's ndcg@3: 0.276852\n",
      "[360]\tvalid_0's ndcg@3: 0.275267\n",
      "[370]\tvalid_0's ndcg@3: 0.275078\n",
      "[380]\tvalid_0's ndcg@3: 0.273769\n",
      "[390]\tvalid_0's ndcg@3: 0.27513\n",
      "[400]\tvalid_0's ndcg@3: 0.276211\n",
      "[410]\tvalid_0's ndcg@3: 0.272626\n",
      "[420]\tvalid_0's ndcg@3: 0.270401\n",
      "[430]\tvalid_0's ndcg@3: 0.271041\n",
      "[440]\tvalid_0's ndcg@3: 0.272289\n",
      "[450]\tvalid_0's ndcg@3: 0.271231\n",
      "[460]\tvalid_0's ndcg@3: 0.271672\n",
      "[470]\tvalid_0's ndcg@3: 0.270615\n",
      "[480]\tvalid_0's ndcg@3: 0.269325\n",
      "[490]\tvalid_0's ndcg@3: 0.27162\n",
      "[500]\tvalid_0's ndcg@3: 0.272208\n",
      "[510]\tvalid_0's ndcg@3: 0.268623\n",
      "[520]\tvalid_0's ndcg@3: 0.270563\n",
      "[530]\tvalid_0's ndcg@3: 0.269116\n",
      "[540]\tvalid_0's ndcg@3: 0.269619\n",
      "[550]\tvalid_0's ndcg@3: 0.270918\n",
      "[560]\tvalid_0's ndcg@3: 0.271412\n",
      "[570]\tvalid_0's ndcg@3: 0.270183\n",
      "[580]\tvalid_0's ndcg@3: 0.27208\n",
      "[590]\tvalid_0's ndcg@3: 0.273171\n",
      "[600]\tvalid_0's ndcg@3: 0.273015\n",
      "[610]\tvalid_0's ndcg@3: 0.273456\n",
      "[620]\tvalid_0's ndcg@3: 0.274087\n",
      "[630]\tvalid_0's ndcg@3: 0.274599\n",
      "[640]\tvalid_0's ndcg@3: 0.274305\n",
      "[650]\tvalid_0's ndcg@3: 0.274409\n",
      "[660]\tvalid_0's ndcg@3: 0.274736\n",
      "[670]\tvalid_0's ndcg@3: 0.273887\n",
      "[680]\tvalid_0's ndcg@3: 0.27374\n",
      "[690]\tvalid_0's ndcg@3: 0.274286\n",
      "[700]\tvalid_0's ndcg@3: 0.275083\n",
      "[710]\tvalid_0's ndcg@3: 0.27439\n",
      "[720]\tvalid_0's ndcg@3: 0.273698\n",
      "[730]\tvalid_0's ndcg@3: 0.273992\n",
      "[740]\tvalid_0's ndcg@3: 0.271853\n",
      "[750]\tvalid_0's ndcg@3: 0.27116\n",
      "[760]\tvalid_0's ndcg@3: 0.271099\n",
      "[770]\tvalid_0's ndcg@3: 0.270207\n",
      "[780]\tvalid_0's ndcg@3: 0.27005\n",
      "[790]\tvalid_0's ndcg@3: 0.270197\n",
      "[800]\tvalid_0's ndcg@3: 0.272242\n",
      "[810]\tvalid_0's ndcg@3: 0.273645\n",
      "[820]\tvalid_0's ndcg@3: 0.272408\n",
      "[830]\tvalid_0's ndcg@3: 0.271317\n",
      "[840]\tvalid_0's ndcg@3: 0.270942\n",
      "[850]\tvalid_0's ndcg@3: 0.271246\n",
      "[860]\tvalid_0's ndcg@3: 0.270207\n",
      "[870]\tvalid_0's ndcg@3: 0.272303\n",
      "[880]\tvalid_0's ndcg@3: 0.273385\n",
      "[890]\tvalid_0's ndcg@3: 0.273551\n",
      "[900]\tvalid_0's ndcg@3: 0.273783\n",
      "[910]\tvalid_0's ndcg@3: 0.272796\n",
      "[920]\tvalid_0's ndcg@3: 0.271966\n",
      "[930]\tvalid_0's ndcg@3: 0.274011\n",
      "[940]\tvalid_0's ndcg@3: 0.274547\n",
      "[950]\tvalid_0's ndcg@3: 0.273309\n",
      "[960]\tvalid_0's ndcg@3: 0.27466\n",
      "[970]\tvalid_0's ndcg@3: 0.274556\n",
      "[980]\tvalid_0's ndcg@3: 0.273404\n",
      "[990]\tvalid_0's ndcg@3: 0.272711\n",
      "[1000]\tvalid_0's ndcg@3: 0.272616\n",
      "Ranker Test Top-3 Recall:  0.38207171314741034\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.metrics import ndcg_score, recall_score\n",
    "from lightgbm import LGBMRanker, log_evaluation, early_stopping\n",
    "# 1. è¯»å–å¹¶é¢„å¤„ç†ï¼ˆè·Ÿä½ ä¹‹å‰çš„ pipeline ä¸€è‡´ï¼Œåªç•™å…³é”®æ­¥éª¤ï¼‰\n",
    "df = pd.read_csv(\"retail_store_sales_cleaned_feature_engineering.csv\",\n",
    "                 parse_dates=[\"Transaction Date\"])\n",
    "df = df.sort_values([\"Customer ID\", \"Transaction Date\"]).reset_index(drop=True)\n",
    "\n",
    "# å‡è®¾ä½ å·²ç»åšå®Œæ‰€æœ‰ç‰¹å¾å·¥ç¨‹ï¼Œå¹¶æŠŠ RFMã€å¤šçª—å£ç‰¹å¾ã€èšç±»ç‰¹å¾éƒ½åŠ è¿›æ¥äº†ï¼š\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"Category_ID\"] = le.fit_transform(df[\"Category\"])\n",
    "# -----------------------------\n",
    "# 3. è¡ç”Ÿç‰¹å¾µï¼šInter_Daysã€30å¤©RFMã€é€±æœŸç·¨ç¢¼ã€flags\n",
    "# -----------------------------\n",
    "df[\"Prev_Date\"] = df.groupby(\"Customer ID\")[\"Transaction Date\"].shift(1)\n",
    "df[\"Inter_Days\"] = (df[\"Transaction Date\"] - df[\"Prev_Date\"])\\\n",
    "                   .dt.days.fillna(0)\n",
    "\n",
    "window_times, window_amounts = deque(), deque()\n",
    "freq30 = np.zeros(len(df), int)\n",
    "amt30_sum = np.zeros(len(df))\n",
    "amt30_count = np.zeros(len(df))\n",
    "\n",
    "for i, (cust, t, amt) in enumerate(zip(\n",
    "    df[\"Customer ID\"],\n",
    "    df[\"Transaction Date\"],\n",
    "    df[\"Total Spent\"]\n",
    ")):\n",
    "    if i == 0 or cust != df.loc[i-1, \"Customer ID\"]:\n",
    "        window_times.clear(); window_amounts.clear()\n",
    "    window_times.append(t); window_amounts.append(amt)\n",
    "    while (t - window_times[0]).days > 30:\n",
    "        window_times.popleft(); window_amounts.popleft()\n",
    "    freq30[i]     = len(window_times) - 1\n",
    "    amt30_sum[i]  = sum(window_amounts)\n",
    "    amt30_count[i]= len(window_times)\n",
    "\n",
    "df[\"freq30\"]     = freq30\n",
    "df[\"amt30_mean\"] = amt30_sum / amt30_count\n",
    "\n",
    "# é€±æœŸç·¨ç¢¼\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"day_sin\"]   = np.sin(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "df[\"day_cos\"]   = np.cos(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "\n",
    "bool_cols = [\n",
    "    \"Is_Weekend\",\"Is_Holiday\",\"Is_NonWorkday\",\n",
    "    \"PM_Credit Card\",\"PM_Digital Wallet\",\n",
    "    \"Loc_Online\",\"Disc_True\",\"Disc_Unknown\"\n",
    "]\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Price Per Unit\",\"Quantity\",\"Total Spent\",\"Recency_Cust\",\n",
    "    \"Inter_Days\",\"freq30\",\"amt30_mean\",\n",
    "    \"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"\n",
    "] + bool_cols\n",
    "\n",
    "def rolling_rfm(df, window_days, amt_col=\"Total Spent\"):\n",
    "    times, amts, freqs, sums = deque(), deque(), [], []\n",
    "    for cust, group in df.groupby(\"Customer ID\", sort=False):\n",
    "        times.clear(); amts.clear()\n",
    "        for t, amt in zip(group[\"Transaction Date\"], group[amt_col]):\n",
    "            times.append(t); amts.append(amt)\n",
    "            # pop è¶…å‡º window_days\n",
    "            while (t - times[0]).days > window_days:\n",
    "                times.popleft(); amts.popleft()\n",
    "            freqs.append(len(times)-1)            # æ‰£æŽ‰è‡ªå·±\n",
    "            sums.append(sum(amts))\n",
    "    return freqs, sums\n",
    "\n",
    "df = df.sort_values([\"Customer ID\",\"Transaction Date\"]).reset_index(drop=True)\n",
    "# 7 å¤©\n",
    "df[\"freq7\"],  df[\"amt7_sum\"]  = rolling_rfm(df,  7)\n",
    "# 30 å¤©ï¼ˆä»¥å‰å·²æœ‰ freq30, amt30_meanï¼‰ï¼Œå¦‚æžœæƒ³ä¿ç•™ sum å¯è¦†è“‹æˆ–æ”¹å\n",
    "# 90 å¤©\n",
    "df[\"freq90\"], df[\"amt90_sum\"] = rolling_rfm(df, 90)\n",
    "\n",
    "# ç‚ºäº†è·Ÿ amt30_mean åŒæ­¥ï¼Œä¸Šé¢ sum æ”¹æˆ mean ä¹Ÿå¾ˆç°¡å–®ï¼š\n",
    "df[\"amt7_mean\"]  = df[\"amt7_sum\"]  / (df[\"freq7\"]  + 1)  # +1 åŒ…å«ç•¶å‰äº¤æ˜“\n",
    "df[\"amt90_mean\"] = df[\"amt90_sum\"] / (df[\"freq90\"] + 1)\n",
    "\n",
    "# --- 2. è·ä»Šæœ€å¾Œè³¼è²·æ—¥çš„æŒ‡æ•¸è¡°æ¸›ç‰¹å¾µï¼ˆRecencyï¼‰ ---------------------\n",
    "\n",
    "# å…ˆç®—ã€Œè·ä»Šå¤©æ•¸ã€\n",
    "today = df[\"Transaction Date\"].max()  # or datetime.today()\n",
    "last_purchase = df.groupby(\"Customer ID\")[\"Transaction Date\"].transform(\"max\")\n",
    "df[\"days_since_last\"] = (today - last_purchase).dt.days\n",
    "\n",
    "# å†åšæŒ‡æ•¸è¡°æ¸›ï¼šexp(-Î»Â·days)ï¼ŒÎ» å¯èª¿\n",
    "lam = 0.05\n",
    "df[\"recency_exp\"] = np.exp(-lam * df[\"days_since_last\"])\n",
    "\n",
    "# --- 3. å®¢æˆ¶æ•´é«” RFM èšé¡ž -----------------------------------------\n",
    "\n",
    "# ç‚ºæ¯ä½å®¢æˆ¶è¨ˆç®—æ•´é«” RFM å‘é‡\n",
    "rfm = df.groupby(\"Customer ID\").agg({\n",
    "    \"days_since_last\": \"min\",               # Recencyï¼šæœ€è¿‘é‚£ç­†äº¤æ˜“è·ä»Š\n",
    "    \"Transaction ID\": \"count\",              # Frequencyï¼šäº¤æ˜“æ¬¡æ•¸\n",
    "    \"Total Spent\": \"sum\"                    # Monetaryï¼šç¸½èŠ±è²»\n",
    "}).rename(columns={\n",
    "    \"days_since_last\": \"R\",\n",
    "    \"Transaction ID\": \"F\",\n",
    "    \"Total Spent\": \"M\"\n",
    "})\n",
    "\n",
    "# æ¨™æº–åŒ–å¾Œåš KMeans\n",
    "scaler_rfm = StandardScaler()\n",
    "rfm_scaled = scaler_rfm.fit_transform(rfm[[\"R\",\"F\",\"M\"]])\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "rfm[\"cluster\"] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Merge å›žåŽŸå§‹ df\n",
    "df = df.merge(rfm[\"cluster\"], left_on=\"Customer ID\", right_index=True)\n",
    "\n",
    "# --- 4. æ›´æ–° feature_cols ---------------------------------------\n",
    "\n",
    "new_feats = [\n",
    "    \"freq7\", \"amt7_mean\",\n",
    "    \"freq90\", \"amt90_mean\",\n",
    "    \"days_since_last\", \"recency_exp\",\n",
    "    \"cluster\"\n",
    "]\n",
    "feature_cols += new_feats\n",
    "# 2. ä¸ºæ¯ä¸ªâ€œæŸ¥è¯¢â€ï¼ˆqueryï¼‰ï¼Œå³æ¯ä¸€æ¬¡è¦é¢„æµ‹ä¸‹ä¸€å“ç±»çš„åŽ†å²åºåˆ—ï¼Œæž„é€ æ­£è´Ÿæ ·æœ¬ï¼š\n",
    "#    æŠŠæ¯æ¡æ­£æ ·æœ¬ (seq â†’ true next Category_ID) å’Œéšæœºé‡‡çš„å‡ ä¸ªè´Ÿæ ·æœ¬æ‹¼åˆ°ä¸€èµ·ï¼Œç”¨åŒä¸€ä¸ª query_id åˆ†ç»„ã€‚\n",
    "records = []\n",
    "for cust, grp in df.groupby(\"Customer ID\"):\n",
    "    cats = grp[\"Category_ID\"].values\n",
    "    feats = grp[feature_cols].values\n",
    "    for i in range(1, len(grp)):\n",
    "        hist_feat = feats[i-1]  # è¿™é‡Œç”¨â€œä¸Šä¸€ç¬”â€ç‰¹å¾ï¼Œä¹Ÿå¯ä»¥ç”¨çª—å£èšåˆç­‰\n",
    "        true_cat  = cats[i]\n",
    "        # æ­£æ ·æœ¬\n",
    "        records.append((cust, hist_feat, true_cat, 1))\n",
    "        # è´Ÿé‡‡æ · 3 ä¸ª\n",
    "        for _ in range(3):\n",
    "            neg = np.random.randint(0, df[\"Category_ID\"].nunique())\n",
    "            while neg == true_cat:\n",
    "                neg = np.random.randint(0, df[\"Category_ID\"].nunique())\n",
    "            records.append((cust, hist_feat, neg, 0))\n",
    "\n",
    "rank_df = pd.DataFrame(records, columns=[\"query_id\",\"feat\",\"cat_id\",\"label\"])\n",
    "\n",
    "# 3. æž„é€ è®­ç»ƒçŸ©é˜µï¼šå°† feat å±•å¼€ã€cat_id ç”¨ one-hot æˆ– embedding ç‰¹å¾æ‹¼è¿›åŽ»\n",
    "#    è¿™é‡Œç¤ºä¾‹ï¼šç›´æŽ¥æŠŠ feat + â€œcat_idâ€ åš one-hotï¼ˆä¹Ÿå¯ç”¨ embedding lookupï¼‰ã€‚\n",
    "X = np.stack(rank_df[\"feat\"].values)\n",
    "# one-hot encode cat_idï¼š\n",
    "\n",
    "# K: ç¸½é¡žåˆ¥æ•¸\n",
    "K = df[\"Category_ID\"].nunique()\n",
    "\n",
    "records = []\n",
    "query_counter = 0\n",
    "\n",
    "for cust, grp in df.groupby(\"Customer ID\", sort=False):\n",
    "    feats = grp[feature_cols].values\n",
    "    cats  = grp[\"Category_ID\"].values\n",
    "    for i in range(1, len(grp)):\n",
    "        hist_feat = feats[i-1]\n",
    "        true_cat  = cats[i]\n",
    "        # é€™ç­†æ­·å²åºåˆ—å°±ç”¨ query_counter ç•¶ä½œå”¯ä¸€ ID\n",
    "        for cat_id in range(K):\n",
    "            label = 1 if cat_id == true_cat else 0\n",
    "            records.append((query_counter, hist_feat, cat_id, label))\n",
    "        query_counter += 1\n",
    "\n",
    "rank_df = pd.DataFrame(records,\n",
    "    columns=[\"query_id\",\"feat\",\"cat_id\",\"label\"])\n",
    "\n",
    "# å±•é–‹ç‰¹å¾µ\n",
    "X = np.vstack(rank_df[\"feat\"].values)\n",
    "onehots = np.eye(K)[rank_df[\"cat_id\"].values]\n",
    "X = np.hstack([X, onehots])\n",
    "y = rank_df[\"label\"].values\n",
    "\n",
    "# æ­£ç¢ºåœ°ç”¨ query_id åˆ†çµ„ï¼Œå¾—åˆ°æ¯ç­† query çš„æ¨£æœ¬æ•¸ï¼ˆæ‡‰è©²éƒ½ç­‰æ–¼ Kï¼‰\n",
    "group_sizes = rank_df.groupby(\"query_id\").size().tolist()  \n",
    "\n",
    "# åˆ‡åˆ† query_ids\n",
    "all_qids = np.arange(query_counter)\n",
    "q_train, q_test = train_test_split(all_qids,\n",
    "    test_size=0.2, random_state=42)\n",
    "\n",
    "# å»ºç«‹è¨“ç·´ï¼æ¸¬è©¦é®ç½©\n",
    "train_mask = rank_df[\"query_id\"].isin(q_train)\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "g_train = [K] * len(q_train)\n",
    "\n",
    "X_test = X[~train_mask]\n",
    "y_test = y[~train_mask]\n",
    "g_test = [K] * len(q_test)\n",
    "# 5. ç”¨ LGBMRanker è®­ç»ƒ\n",
    "callbacks = [\n",
    "    log_evaluation(period=10),\n",
    "\n",
    "]\n",
    "\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    random_state=42,\n",
    "    importance_type=\"gain\"\n",
    ")\n",
    "\n",
    "ranker.fit(\n",
    "    X_train, y_train,\n",
    "    group=g_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_group=[g_test],\n",
    "    eval_at=[3],          # NDCG@3\n",
    "    callbacks=callbacks    # â† åœ¨è¿™é‡Œä¼ å…¥ callbacksï¼Œå–ä»£ verbose\n",
    ")\n",
    "# 6. è¯„ä¼° Top-3 Recall\n",
    "#    å¯¹æµ‹è¯•æ¯ä¸ª queryï¼Œpredict å¾—åˆ†åŽå– top3ï¼Œçœ‹æ­£æ ·æœ¬æ˜¯å¦åœ¨å†…\n",
    "pred_scores = ranker.predict(X_test)\n",
    "# é‡ç»„ä¸º per-query scores\n",
    "offset = 0\n",
    "hits = []\n",
    "for grp_size in g_test:\n",
    "    grp_scores = pred_scores[offset:offset+grp_size]\n",
    "    grp_labels = y_test      [offset:offset+grp_size]\n",
    "    topk_idx   = np.argsort(grp_scores)[-3:]\n",
    "    # åªè¦æ­£æ ·æœ¬(label=1)åœ¨ top3 å°±ç®—å‘½ä¸­\n",
    "    hits.append(any(grp_labels[i]==1 for i in topk_idx))\n",
    "    offset += grp_size\n",
    "\n",
    "print(\"Ranker Test Top-3 Recall: \", np.mean(hits))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
