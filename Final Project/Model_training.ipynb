{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbe1b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307633f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d03f31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [00:23:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                         Beverages       0.10      0.10      0.10       308\n",
      "                          Butchers       0.13      0.13      0.13       305\n",
      "Computers and electric accessories       0.14      0.14      0.14       305\n",
      "     Electric household essentials       0.12      0.12      0.12       312\n",
      "                              Food       0.10      0.10      0.10       313\n",
      "                         Furniture       0.11      0.11      0.11       310\n",
      "                     Milk Products       0.13      0.13      0.13       308\n",
      "                        Patisserie       0.12      0.11      0.11       299\n",
      "\n",
      "                          accuracy                           0.12      2460\n",
      "                         macro avg       0.12      0.12      0.12      2460\n",
      "                      weighted avg       0.12      0.12      0.12      2460\n",
      "\n",
      "\n",
      "類別編碼映射:\n",
      "0: Beverages\n",
      "1: Butchers\n",
      "2: Computers and electric accessories\n",
      "3: Electric household essentials\n",
      "4: Food\n",
      "5: Furniture\n",
      "6: Milk Products\n",
      "7: Patisserie\n",
      "Top-1 準確率: 0.1167\n",
      "Top-3 準確率: 0.3817\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('retail_store_sales_cleaned_feature_engineering.csv')\n",
    "# 原始排序\n",
    "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
    "df = df.sort_values(['Customer ID', 'Transaction Date']).reset_index(drop=True)\n",
    "\n",
    "# 建立 Category LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Category_ID'] = le.fit_transform(df['Category'])\n",
    "\n",
    "# 建立嵌入矩陣（例如每個類別對應 8 維向量）\n",
    "NUM_CATEGORIES = df['Category_ID'].nunique()\n",
    "EMBEDDING_DIM = 16\n",
    "embedding_layer = nn.Embedding(NUM_CATEGORIES, EMBEDDING_DIM)\n",
    "\n",
    "# 記錄所有特徵向量\n",
    "feature_rows = []\n",
    "\n",
    "# 設定 N：考慮過去 N 筆購買行為\n",
    "N = 10\n",
    "\n",
    "# 在原有特徵基礎上，新增更多時間序列特徵\n",
    "for cust_id, cust_df in df.groupby('Customer ID'):\n",
    "    cust_df = cust_df.sort_values('Transaction Date')\n",
    "    for i in range(N, len(cust_df) - 1):\n",
    "        history = cust_df.iloc[i-N:i]\n",
    "        current = cust_df.iloc[i]\n",
    "        next_row = cust_df.iloc[i + 1]\n",
    "\n",
    "        # 嵌入特徵\n",
    "        cat_ids = torch.tensor(history['Category_ID'].tolist(), dtype=torch.long)\n",
    "        embedded = embedding_layer(cat_ids)\n",
    "        embedded_flat = embedded.flatten().detach().numpy()\n",
    "\n",
    "        # 增強的特徵工程\n",
    "        row = {\n",
    "            # 原有特徵\n",
    "            'TotalSpent_Mean': history['Total Spent'].mean(),\n",
    "            'TotalSpent_Std': history['Total Spent'].std(),\n",
    "            'TotalSpent_Last': history['Total Spent'].iloc[-1],\n",
    "            'Quantity_Mean': history['Quantity'].mean(),\n",
    "            'Quantity_Sum': history['Quantity'].sum(),\n",
    "            'Discount_Used_Count': history['Disc_True'].sum(),\n",
    "            'Discount_Rate': history['Disc_True'].mean(),\n",
    "            \n",
    "            # 時間特徵\n",
    "            'Recency_Days': (current['Transaction Date'] - history['Transaction Date'].max()).days,\n",
    "            'Frequency': len(history),\n",
    "            'Days_Between_Purchases': history['Transaction Date'].diff().dt.days.mean(),\n",
    "            \n",
    "            # 類別序列特徵\n",
    "            'Most_Frequent_Category': history['Category'].mode().iloc[0] if not history['Category'].mode().empty else 'Unknown',\n",
    "            'Category_Diversity': history['Category'].nunique(),\n",
    "            'Last_Category': history['Category'].iloc[-1],\n",
    "            'Category_Repeat_Rate': (history['Category'] == history['Category'].iloc[-1]).mean(),\n",
    "            \n",
    "            # 現在的交易特徵\n",
    "            'Is_Weekend': current['Is_Weekend'],\n",
    "            'Is_Holiday': current['Is_Holiday'],\n",
    "            'Is_NonWorkday': current['Is_NonWorkday'],\n",
    "            'PM_Credit Card': current['PM_Credit Card'],\n",
    "            'PM_Digital Wallet': current['PM_Digital Wallet'],\n",
    "            'Loc_Online': current['Loc_Online'],\n",
    "            'Disc_True': current['Disc_True'],\n",
    "            'Disc_Unknown': current['Disc_Unknown'],\n",
    "            'Current_Category': current['Category'],\n",
    "            'Next_Category': next_row['Category']\n",
    "        }\n",
    "\n",
    "        # 拼接嵌入特徵\n",
    "        for j, val in enumerate(embedded_flat):\n",
    "            row[f'Embed_{j}'] = val\n",
    "\n",
    "        feature_rows.append(row)\n",
    "        \n",
    "\n",
    "feature_df = pd.DataFrame(feature_rows).fillna(0)\n",
    "\n",
    "# 對目標變數進行標籤編碼\n",
    "y_encoder = LabelEncoder()\n",
    "feature_df['Next_Category_Encoded'] = y_encoder.fit_transform(feature_df['Next_Category'])\n",
    "\n",
    "# 對字串類型的特徵進行編碼\n",
    "category_encoder = LabelEncoder()\n",
    "feature_df['Most_Frequent_Category_Encoded'] = category_encoder.fit_transform(feature_df['Most_Frequent_Category'])\n",
    "\n",
    "last_category_encoder = LabelEncoder()\n",
    "feature_df['Last_Category_Encoded'] = last_category_encoder.fit_transform(feature_df['Last_Category'])\n",
    "\n",
    "# 移除原始字串特徵，保留編碼後的特徵\n",
    "feature_df = feature_df.drop(columns=['Most_Frequent_Category', 'Last_Category'])\n",
    "\n",
    "# One-hot encode 現在購買的類別\n",
    "X = pd.get_dummies(feature_df.drop(columns=['Next_Category', 'Next_Category_Encoded']), columns=['Current_Category'])\n",
    "y = feature_df['Next_Category_Encoded']  # 使用編碼後的標籤\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost 模型訓練\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 預測與評估\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 將預測結果轉換回原始類別名稱以便查看\n",
    "y_test_labels = y_encoder.inverse_transform(y_test)\n",
    "y_pred_labels = y_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "\n",
    "# 顯示類別映射\n",
    "print(\"\\n類別編碼映射:\")\n",
    "for i, category in enumerate(y_encoder.classes_):\n",
    "    print(f\"{i}: {category}\")\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# 使用 sklearn 的 top_k_accuracy_score\n",
    "top1_acc = top_k_accuracy_score(y_test, y_pred_proba, k=1)\n",
    "top3_acc = top_k_accuracy_score(y_test, y_pred_proba, k=3)\n",
    "\n",
    "print(f\"Top-1 準確率: {top1_acc:.4f}\")\n",
    "print(f\"Top-3 準確率: {top3_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414510f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有顧客中最多的購買次數是：544\n"
     ]
    }
   ],
   "source": [
    "# 計算每位顧客的購買次數\n",
    "cust_counts = df.groupby('Customer ID').size()\n",
    "\n",
    "# 最大購買次數\n",
    "max_n = cust_counts.max()\n",
    "print(f\"✅ 所有顧客中最多的購買次數是：{max_n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8421b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 你最多可以設 N 為：543\n"
     ]
    }
   ],
   "source": [
    "purchase_counts = df.groupby('Customer ID').size()\n",
    "max_available_N = purchase_counts.max() - 1  # 最長可產生序列的長度\n",
    "\n",
    "print(f\"🔧 你最多可以設 N 為：{max_available_N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7fa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Total training samples created: 232\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da46a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754d2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Total training samples created: 12550\n",
      "\n",
      "🚀 Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     95\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m---> 96\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[1;32m     97\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "PAD_TOKEN = -1\n",
    "df = pd.read_csv('retail_store_sales_cleaned_feature_engineering.csv')\n",
    "# =======================\n",
    "# 資料處理：組成序列資料\n",
    "# =======================\n",
    "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'])\n",
    "df = df.sort_values(['Customer ID', 'Transaction Date']).reset_index(drop=True)\n",
    "\n",
    "# 編碼類別\n",
    "le = LabelEncoder()\n",
    "df['Category_ID'] = le.fit_transform(df['Category'])\n",
    "num_classes = df['Category_ID'].nunique()\n",
    "\n",
    "\n",
    "\n",
    "PAD_TOKEN = df['Category_ID'].nunique()  # 類別數，保證 PAD 不重複\n",
    "N = 500  # 固定序列長度\n",
    "seq_data = []\n",
    "\n",
    "for cust_id, cust_df in df.groupby('Customer ID'):\n",
    "    category_ids = cust_df['Category_ID'].tolist()\n",
    "    T = len(category_ids)\n",
    "\n",
    "    for i in range(1, T):  # 每一筆要預測第 i 筆（從第 1 筆開始）\n",
    "        history = category_ids[:i]           # 顧客第 i 筆前的所有紀錄\n",
    "        padded = [PAD_TOKEN] * max(0, N - len(history)) + history[-N:]  # 補 PAD 至長度 N\n",
    "        label = category_ids[i]              # 第 i 筆的類別就是 label\n",
    "        seq_data.append((padded, label))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"📊 Total training samples created: {len(seq_data)}\")\n",
    "# 分訓練測試集\n",
    "X_seq, y_seq = zip(*seq_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(X_seq), list(y_seq), test_size=0.1, stratify=y_seq)\n",
    "\n",
    "# =======================\n",
    "# 建立 Dataset & DataLoader\n",
    "# =======================\n",
    "class PurchaseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.LongTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = PurchaseDataset(X_train, y_train)\n",
    "test_ds = PurchaseDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "# =======================\n",
    "# 建立 LSTM 模型\n",
    "# =======================\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, num_classes, emb_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_classes + 1, embedding_dim=32, padding_idx=PAD_TOKEN)\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch, seq_len, emb_dim)\n",
    "        _, (h_n, _) = self.lstm(x)  # 只取最後一層 hidden\n",
    "        out = self.fc(h_n.squeeze(0))  # (batch, num_classes)\n",
    "        return out\n",
    "\n",
    "# =======================\n",
    "# 模型訓練\n",
    "# =======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMPredictor(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top3 = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    print(f\"\\n🚀 Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)  # (batch_size, num_classes)\n",
    "\n",
    "        # 計算損失\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # -------- 準確率計算 --------\n",
    "        _, pred_top1 = torch.max(outputs, dim=1)\n",
    "        correct_top1 += (pred_top1 == y_batch).sum().item()\n",
    "\n",
    "        top3 = torch.topk(outputs, 3, dim=1).indices\n",
    "        for i in range(len(y_batch)):\n",
    "            if y_batch[i] in top3[i]:\n",
    "                correct_top3 += 1\n",
    "\n",
    "        total_samples += y_batch.size(0)\n",
    "        # ---------------------------\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    top1_acc = correct_top1 / total_samples\n",
    "    top3_acc = correct_top3 / total_samples\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1} - Loss: {avg_loss:.4f} | Top-1 Accuracy: {top1_acc:.2%} | Top-3 Accuracy: {top3_acc:.2%}\")\n",
    "# =======================\n",
    "# Top-3 預測與評估\n",
    "# =======================\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        logits = model(X_batch)\n",
    "        top3 = torch.topk(logits, 3, dim=1).indices.cpu().numpy()\n",
    "        all_preds.extend(top3)\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "# Top-3 命中率\n",
    "hits = [label in pred for label, pred in zip(all_labels, all_preds)]\n",
    "top3_acc = np.mean(hits)\n",
    "print(f\"\\n✅ Top-3 Accuracy: {top3_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afdcdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m202.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  lightgbm\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74baa4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning LogisticRegression ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning RandomForest ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      ">> Tuning HistGradientBoost ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      ">> Tuning ExtraTrees ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning LightGBM ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002555 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007740 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006377 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009535 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010718 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002418 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 690\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 691\n",
      "[LightGBM] [Info] Number of data points in the train set: 10049, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score -2.093169\n",
      "[LightGBM] [Info] Start training from score -2.069244\n",
      "[LightGBM] [Info] Start training from score -2.107803\n",
      "[LightGBM] [Info] Start training from score -2.053606\n",
      "[LightGBM] [Info] Start training from score -2.082731\n",
      "[LightGBM] [Info] Start training from score -2.060613\n",
      "[LightGBM] [Info] Start training from score -2.064527\n",
      "[LightGBM] [Info] Start training from score -2.105349\n",
      "\n",
      ">> Tuning XGBoost ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:06:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [16:07:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Comparison ===\n",
      "             Model                                                                                                  BestParams Val_Recall@3 Test_Acc Test_Recall@3\n",
      "LogisticRegression                                               {'C': 2.1333911067827613, 'penalty': 'l2', 'solver': 'lbfgs'}       39.54%   14.57%        39.55%\n",
      "      RandomForest                                                 {'max_depth': 6, 'max_features': None, 'n_estimators': 199}       51.65%   20.78%        53.84%\n",
      " HistGradientBoost                              {'learning_rate': 0.010233629752304298, 'max_iter': 237, 'max_leaf_nodes': 30}       49.05%   19.91%        50.71%\n",
      "        ExtraTrees                                                 {'max_depth': 6, 'max_features': None, 'n_estimators': 199}       52.86%   20.86%        53.64%\n",
      "          LightGBM                              {'learning_rate': 0.010233629752304298, 'n_estimators': 237, 'num_leaves': 30}       49.36%   18.49%        50.91%\n",
      "           XGBoost {'learning_rate': 0.05958008171890075, 'max_depth': 9, 'n_estimators': 58, 'subsample': 0.8861223846483287}       48.10%   17.97%        50.00%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 讀檔 + 時間排序\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\n",
    "    \"retail_store_sales_cleaned_feature_engineering.csv\",\n",
    "    parse_dates=[\"Transaction Date\"]\n",
    ")\n",
    "df = df.sort_values([\"Customer ID\", \"Transaction Date\"])\\\n",
    "       .reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 目標編碼\n",
    "# -----------------------------\n",
    "le = LabelEncoder()\n",
    "df[\"Category_ID\"] = le.fit_transform(df[\"Category\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 衍生特徵：Inter_Days、30天RFM、週期編碼、flags\n",
    "# -----------------------------\n",
    "df[\"Prev_Date\"] = df.groupby(\"Customer ID\")[\"Transaction Date\"].shift(1)\n",
    "df[\"Inter_Days\"] = (df[\"Transaction Date\"] - df[\"Prev_Date\"])\\\n",
    "                   .dt.days.fillna(0)\n",
    "\n",
    "window_times, window_amounts = deque(), deque()\n",
    "freq30 = np.zeros(len(df), int)\n",
    "amt30_sum = np.zeros(len(df))\n",
    "amt30_count = np.zeros(len(df))\n",
    "\n",
    "for i, (cust, t, amt) in enumerate(zip(\n",
    "    df[\"Customer ID\"],\n",
    "    df[\"Transaction Date\"],\n",
    "    df[\"Total Spent\"]\n",
    ")):\n",
    "    if i == 0 or cust != df.loc[i-1, \"Customer ID\"]:\n",
    "        window_times.clear(); window_amounts.clear()\n",
    "    window_times.append(t); window_amounts.append(amt)\n",
    "    while (t - window_times[0]).days > 30:\n",
    "        window_times.popleft(); window_amounts.popleft()\n",
    "    freq30[i]     = len(window_times) - 1\n",
    "    amt30_sum[i]  = sum(window_amounts)\n",
    "    amt30_count[i]= len(window_times)\n",
    "\n",
    "df[\"freq30\"]     = freq30\n",
    "df[\"amt30_mean\"] = amt30_sum / amt30_count\n",
    "\n",
    "# 週期編碼\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"day_sin\"]   = np.sin(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "df[\"day_cos\"]   = np.cos(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "\n",
    "bool_cols = [\n",
    "    \"Is_Weekend\",\"Is_Holiday\",\"Is_NonWorkday\",\n",
    "    \"PM_Credit Card\",\"PM_Digital Wallet\",\n",
    "    \"Loc_Online\",\"Disc_True\",\"Disc_Unknown\"\n",
    "]\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Price Per Unit\",\"Quantity\",\"Total Spent\",\"Recency_Cust\",\n",
    "    \"Inter_Days\",\"freq30\",\"amt30_mean\",\n",
    "    \"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"\n",
    "] + bool_cols\n",
    "\n",
    "# -----------------------------\n",
    "# 4. per-user 時間切分 80%/20%\n",
    "# -----------------------------\n",
    "train_list, test_list = [], []\n",
    "for _, grp in df.groupby(\"Customer ID\"):\n",
    "    grp = grp.sort_values(\"Transaction Date\").reset_index(drop=True)\n",
    "    cut = int(len(grp) * 0.8)\n",
    "    train_list.append(grp.iloc[:cut])\n",
    "    test_list.append(grp.iloc[cut:])\n",
    "\n",
    "train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "test_df  = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. 標準化\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df [feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[\"Category_ID\"]\n",
    "X_test,  y_test  = test_df [feature_cols], test_df [\"Category_ID\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Recall@3 打分函式（先轉成 numpy）\n",
    "# -----------------------------\n",
    "def recall_at_3(y_true, y_score):\n",
    "    y_true = np.array(y_true)  # <- 這行很重要\n",
    "    top3 = np.argsort(y_score, axis=1)[:, -3:]\n",
    "    return np.mean([y_true[i] in top3[i] for i in range(len(y_true))])\n",
    "\n",
    "recall3_scorer = make_scorer(recall_at_3, needs_proba=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. 模型與超參搜尋空間\n",
    "# -----------------------------\n",
    "models_and_spaces = {\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(random_state=42, max_iter=1000),\n",
    "        {\n",
    "            \"C\": uniform(0.01, 10),\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"solver\": [\"lbfgs\"]\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"max_depth\": randint(3, 20),\n",
    "            \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "        }\n",
    "    ),\n",
    "    \"HistGradientBoost\": (\n",
    "        HistGradientBoostingClassifier(random_state=42),\n",
    "        {\n",
    "            \"learning_rate\": uniform(0.01, 0.3),\n",
    "            \"max_iter\": randint(50, 300),\n",
    "            \"max_leaf_nodes\": randint(10, 100)\n",
    "        }\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"max_depth\": randint(3, 20),\n",
    "            \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"learning_rate\": uniform(0.01, 0.3),\n",
    "            \"num_leaves\": randint(10, 150)\n",
    "        }\n",
    "    ),\n",
    "    # \"XGBoost\": (\n",
    "    #     XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "    #     {\n",
    "    #         \"n_estimators\": randint(50, 300),\n",
    "    #         \"learning_rate\": uniform(0.01, 0.3),\n",
    "    #         \"max_depth\": randint(3, 20),\n",
    "    #         \"subsample\": uniform(0.5, 0.5)\n",
    "    #     }\n",
    "    # )\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 8. RandomizedSearchCV\n",
    "# -----------------------------\n",
    "best_results = []\n",
    "for name, (estimator, param_dist) in models_and_spaces.items():\n",
    "    print(f\"\\n>> Tuning {name} ...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring={\"acc\": \"accuracy\", \"rec3\": recall3_scorer},\n",
    "        refit=\"rec3\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best = search.best_estimator_\n",
    "    test_acc  = accuracy_score(y_test, best.predict(X_test))\n",
    "    test_rec3 = recall_at_3(y_test, best.predict_proba(X_test))\n",
    "\n",
    "    best_results.append({\n",
    "        \"Model\": name,\n",
    "        \"BestParams\": search.best_params_,\n",
    "        \"Val_Recall@3\": f\"{search.best_score_:.2%}\",\n",
    "        \"Test_Acc\": f\"{test_acc:.2%}\",\n",
    "        \"Test_Recall@3\": f\"{test_rec3:.2%}\"\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# 9. 列印結果\n",
    "# -----------------------------\n",
    "results_df = pd.DataFrame(best_results)\n",
    "print(\"\\n=== Final Comparison ===\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02e18bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BestParams</th>\n",
       "      <th>Val_Recall@3</th>\n",
       "      <th>Test_Acc</th>\n",
       "      <th>Test_Recall@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 2.1333911067827613, 'penalty': 'l2', 'so...</td>\n",
       "      <td>39.54%</td>\n",
       "      <td>14.57%</td>\n",
       "      <td>39.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'max_depth': 6, 'max_features': None, 'n_esti...</td>\n",
       "      <td>51.65%</td>\n",
       "      <td>20.78%</td>\n",
       "      <td>53.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HistGradientBoost</td>\n",
       "      <td>{'learning_rate': 0.010233629752304298, 'max_i...</td>\n",
       "      <td>49.05%</td>\n",
       "      <td>19.91%</td>\n",
       "      <td>50.71%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'max_depth': 6, 'max_features': None, 'n_esti...</td>\n",
       "      <td>52.86%</td>\n",
       "      <td>20.86%</td>\n",
       "      <td>53.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'learning_rate': 0.010233629752304298, 'n_est...</td>\n",
       "      <td>49.36%</td>\n",
       "      <td>18.49%</td>\n",
       "      <td>50.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'learning_rate': 0.05958008171890075, 'max_de...</td>\n",
       "      <td>48.10%</td>\n",
       "      <td>17.97%</td>\n",
       "      <td>50.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model                                         BestParams  \\\n",
       "0  LogisticRegression  {'C': 2.1333911067827613, 'penalty': 'l2', 'so...   \n",
       "1        RandomForest  {'max_depth': 6, 'max_features': None, 'n_esti...   \n",
       "2   HistGradientBoost  {'learning_rate': 0.010233629752304298, 'max_i...   \n",
       "3          ExtraTrees  {'max_depth': 6, 'max_features': None, 'n_esti...   \n",
       "4            LightGBM  {'learning_rate': 0.010233629752304298, 'n_est...   \n",
       "5             XGBoost  {'learning_rate': 0.05958008171890075, 'max_de...   \n",
       "\n",
       "  Val_Recall@3 Test_Acc Test_Recall@3  \n",
       "0       39.54%   14.57%        39.55%  \n",
       "1       51.65%   20.78%        53.84%  \n",
       "2       49.05%   19.91%        50.71%  \n",
       "3       52.86%   20.86%        53.64%  \n",
       "4       49.36%   18.49%        50.91%  \n",
       "5       48.10%   17.97%        50.00%  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9358bf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning LogisticRegression ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning RandomForest ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      ">> Tuning HistGradientBoost ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      ">> Tuning ExtraTrees ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Tuning LightGBM ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Start training from score -2.069194[LightGBM] [Info] Total Bins 1296\n",
      "\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.082282[LightGBM] [Info] Start training from score -2.069194\n",
      "\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.105299[LightGBM] [Info] Start training from score -2.083480\n",
      "\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006573 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047858 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009538 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenthsia/miniconda3/envs/combine/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001479 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007233 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.053945\n",
      "[LightGBM] [Info] Start training from score -2.082282\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1296\n",
      "[LightGBM] [Info] Number of data points in the train set: 6699, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093119\n",
      "[LightGBM] [Info] Start training from score -2.069194\n",
      "[LightGBM] [Info] Start training from score -2.107753\n",
      "[LightGBM] [Info] Start training from score -2.052781\n",
      "[LightGBM] [Info] Start training from score -2.083480\n",
      "[LightGBM] [Info] Start training from score -2.060954\n",
      "[LightGBM] [Info] Start training from score -2.064477\n",
      "[LightGBM] [Info] Start training from score -2.105299\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 6700, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093268\n",
      "[LightGBM] [Info] Start training from score -2.069343\n",
      "[LightGBM] [Info] Start training from score -2.107902\n",
      "[LightGBM] [Info] Start training from score -2.054094\n",
      "[LightGBM] [Info] Start training from score -2.082431\n",
      "[LightGBM] [Info] Start training from score -2.059932\n",
      "[LightGBM] [Info] Start training from score -2.064626\n",
      "[LightGBM] [Info] Start training from score -2.105448\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1297\n",
      "[LightGBM] [Info] Number of data points in the train set: 10049, number of used features: 26\n",
      "[LightGBM] [Info] Start training from score -2.093169\n",
      "[LightGBM] [Info] Start training from score -2.069244\n",
      "[LightGBM] [Info] Start training from score -2.107803\n",
      "[LightGBM] [Info] Start training from score -2.053606\n",
      "[LightGBM] [Info] Start training from score -2.082731\n",
      "[LightGBM] [Info] Start training from score -2.060613\n",
      "[LightGBM] [Info] Start training from score -2.064527\n",
      "[LightGBM] [Info] Start training from score -2.105349\n",
      "\n",
      "=== Final Comparison ===\n",
      "             Model                                                                     BestParams Val_Recall@3 Test_Acc Test_Recall@3\n",
      "LogisticRegression                 {'C': 0.21584494295802448, 'penalty': 'l2', 'solver': 'lbfgs'}       39.89%   13.66%        40.02%\n",
      "      RandomForest                    {'max_depth': 6, 'max_features': None, 'n_estimators': 199}       51.17%   20.55%        53.52%\n",
      " HistGradientBoost {'learning_rate': 0.010233629752304298, 'max_iter': 237, 'max_leaf_nodes': 30}       48.19%   18.69%        50.36%\n",
      "        ExtraTrees                    {'max_depth': 6, 'max_features': None, 'n_estimators': 199}       52.37%   20.59%        53.92%\n",
      "          LightGBM {'learning_rate': 0.010233629752304298, 'n_estimators': 237, 'num_leaves': 30}       48.24%   18.05%        50.36%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 讀檔 + 時間排序\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\n",
    "    \"retail_store_sales_cleaned_feature_engineering.csv\",\n",
    "    parse_dates=[\"Transaction Date\"]\n",
    ")\n",
    "df = df.sort_values([\"Customer ID\", \"Transaction Date\"])\\\n",
    "       .reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 目標編碼\n",
    "# -----------------------------\n",
    "le = LabelEncoder()\n",
    "df[\"Category_ID\"] = le.fit_transform(df[\"Category\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 衍生特徵：Inter_Days、30天RFM、週期編碼、flags\n",
    "# -----------------------------\n",
    "df[\"Prev_Date\"] = df.groupby(\"Customer ID\")[\"Transaction Date\"].shift(1)\n",
    "df[\"Inter_Days\"] = (df[\"Transaction Date\"] - df[\"Prev_Date\"])\\\n",
    "                   .dt.days.fillna(0)\n",
    "\n",
    "window_times, window_amounts = deque(), deque()\n",
    "freq30 = np.zeros(len(df), int)\n",
    "amt30_sum = np.zeros(len(df))\n",
    "amt30_count = np.zeros(len(df))\n",
    "\n",
    "for i, (cust, t, amt) in enumerate(zip(\n",
    "    df[\"Customer ID\"],\n",
    "    df[\"Transaction Date\"],\n",
    "    df[\"Total Spent\"]\n",
    ")):\n",
    "    if i == 0 or cust != df.loc[i-1, \"Customer ID\"]:\n",
    "        window_times.clear(); window_amounts.clear()\n",
    "    window_times.append(t); window_amounts.append(amt)\n",
    "    while (t - window_times[0]).days > 30:\n",
    "        window_times.popleft(); window_amounts.popleft()\n",
    "    freq30[i]     = len(window_times) - 1\n",
    "    amt30_sum[i]  = sum(window_amounts)\n",
    "    amt30_count[i]= len(window_times)\n",
    "\n",
    "df[\"freq30\"]     = freq30\n",
    "df[\"amt30_mean\"] = amt30_sum / amt30_count\n",
    "\n",
    "# 週期編碼\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"day_sin\"]   = np.sin(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "df[\"day_cos\"]   = np.cos(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "\n",
    "bool_cols = [\n",
    "    \"Is_Weekend\",\"Is_Holiday\",\"Is_NonWorkday\",\n",
    "    \"PM_Credit Card\",\"PM_Digital Wallet\",\n",
    "    \"Loc_Online\",\"Disc_True\",\"Disc_Unknown\"\n",
    "]\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Price Per Unit\",\"Quantity\",\"Total Spent\",\"Recency_Cust\",\n",
    "    \"Inter_Days\",\"freq30\",\"amt30_mean\",\n",
    "    \"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"\n",
    "] + bool_cols\n",
    "\n",
    "def rolling_rfm(df, window_days, amt_col=\"Total Spent\"):\n",
    "    times, amts, freqs, sums = deque(), deque(), [], []\n",
    "    for cust, group in df.groupby(\"Customer ID\", sort=False):\n",
    "        times.clear(); amts.clear()\n",
    "        for t, amt in zip(group[\"Transaction Date\"], group[amt_col]):\n",
    "            times.append(t); amts.append(amt)\n",
    "            # pop 超出 window_days\n",
    "            while (t - times[0]).days > window_days:\n",
    "                times.popleft(); amts.popleft()\n",
    "            freqs.append(len(times)-1)            # 扣掉自己\n",
    "            sums.append(sum(amts))\n",
    "    return freqs, sums\n",
    "\n",
    "df = df.sort_values([\"Customer ID\",\"Transaction Date\"]).reset_index(drop=True)\n",
    "# 7 天\n",
    "df[\"freq7\"],  df[\"amt7_sum\"]  = rolling_rfm(df,  7)\n",
    "# 30 天（以前已有 freq30, amt30_mean），如果想保留 sum 可覆蓋或改名\n",
    "# 90 天\n",
    "df[\"freq90\"], df[\"amt90_sum\"] = rolling_rfm(df, 90)\n",
    "\n",
    "# 為了跟 amt30_mean 同步，上面 sum 改成 mean 也很簡單：\n",
    "df[\"amt7_mean\"]  = df[\"amt7_sum\"]  / (df[\"freq7\"]  + 1)  # +1 包含當前交易\n",
    "df[\"amt90_mean\"] = df[\"amt90_sum\"] / (df[\"freq90\"] + 1)\n",
    "\n",
    "# --- 2. 距今最後購買日的指數衰減特徵（Recency） ---------------------\n",
    "\n",
    "# 先算「距今天數」\n",
    "today = df[\"Transaction Date\"].max()  # or datetime.today()\n",
    "last_purchase = df.groupby(\"Customer ID\")[\"Transaction Date\"].transform(\"max\")\n",
    "df[\"days_since_last\"] = (today - last_purchase).dt.days\n",
    "\n",
    "# 再做指數衰減：exp(-λ·days)，λ 可調\n",
    "lam = 0.05\n",
    "df[\"recency_exp\"] = np.exp(-lam * df[\"days_since_last\"])\n",
    "\n",
    "# --- 3. 客戶整體 RFM 聚類 -----------------------------------------\n",
    "\n",
    "# 為每位客戶計算整體 RFM 向量\n",
    "rfm = df.groupby(\"Customer ID\").agg({\n",
    "    \"days_since_last\": \"min\",               # Recency：最近那筆交易距今\n",
    "    \"Transaction ID\": \"count\",              # Frequency：交易次數\n",
    "    \"Total Spent\": \"sum\"                    # Monetary：總花費\n",
    "}).rename(columns={\n",
    "    \"days_since_last\": \"R\",\n",
    "    \"Transaction ID\": \"F\",\n",
    "    \"Total Spent\": \"M\"\n",
    "})\n",
    "\n",
    "# 標準化後做 KMeans\n",
    "scaler_rfm = StandardScaler()\n",
    "rfm_scaled = scaler_rfm.fit_transform(rfm[[\"R\",\"F\",\"M\"]])\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "rfm[\"cluster\"] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Merge 回原始 df\n",
    "df = df.merge(rfm[\"cluster\"], left_on=\"Customer ID\", right_index=True)\n",
    "\n",
    "# --- 4. 更新 feature_cols ---------------------------------------\n",
    "\n",
    "new_feats = [\n",
    "    \"freq7\", \"amt7_mean\",\n",
    "    \"freq90\", \"amt90_mean\",\n",
    "    \"days_since_last\", \"recency_exp\",\n",
    "    \"cluster\"\n",
    "]\n",
    "feature_cols += new_feats\n",
    "\n",
    "\n",
    "train_list, test_list = [], []\n",
    "for _, grp in df.groupby(\"Customer ID\"):\n",
    "    grp = grp.sort_values(\"Transaction Date\").reset_index(drop=True)\n",
    "    cut = int(len(grp) * 0.8)\n",
    "    train_list.append(grp.iloc[:cut])\n",
    "    test_list.append(grp.iloc[cut:])\n",
    "\n",
    "train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "test_df  = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. 標準化\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
    "test_df [feature_cols] = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "X_train, y_train = train_df[feature_cols], train_df[\"Category_ID\"]\n",
    "X_test,  y_test  = test_df [feature_cols], test_df [\"Category_ID\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Recall@3 打分函式（先轉成 numpy）\n",
    "# -----------------------------\n",
    "def recall_at_3(y_true, y_score):\n",
    "    y_true = np.array(y_true)  # <- 這行很重要\n",
    "    top3 = np.argsort(y_score, axis=1)[:, -3:]\n",
    "    return np.mean([y_true[i] in top3[i] for i in range(len(y_true))])\n",
    "\n",
    "recall3_scorer = make_scorer(recall_at_3, needs_proba=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. 模型與超參搜尋空間\n",
    "# -----------------------------\n",
    "models_and_spaces = {\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(random_state=42, max_iter=1000),\n",
    "        {\n",
    "            \"C\": uniform(0.01, 10),\n",
    "            \"penalty\": [\"l2\"],\n",
    "            \"solver\": [\"lbfgs\"]\n",
    "        }\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"max_depth\": randint(3, 20),\n",
    "            \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "        }\n",
    "    ),\n",
    "    \"HistGradientBoost\": (\n",
    "        HistGradientBoostingClassifier(random_state=42),\n",
    "        {\n",
    "            \"learning_rate\": uniform(0.01, 0.3),\n",
    "            \"max_iter\": randint(50, 300),\n",
    "            \"max_leaf_nodes\": randint(10, 100)\n",
    "        }\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"max_depth\": randint(3, 20),\n",
    "            \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        LGBMClassifier(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": randint(50, 300),\n",
    "            \"learning_rate\": uniform(0.01, 0.3),\n",
    "            \"num_leaves\": randint(10, 150)\n",
    "        }\n",
    "    ),\n",
    "    # \"XGBoost\": (\n",
    "    #     XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"mlogloss\"),\n",
    "    #     {\n",
    "    #         \"n_estimators\": randint(50, 300),\n",
    "    #         \"learning_rate\": uniform(0.01, 0.3),\n",
    "    #         \"max_depth\": randint(3, 20),\n",
    "    #         \"subsample\": uniform(0.5, 0.5)\n",
    "    #     }\n",
    "    # )\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 8. RandomizedSearchCV\n",
    "# -----------------------------\n",
    "best_results = []\n",
    "for name, (estimator, param_dist) in models_and_spaces.items():\n",
    "    print(f\"\\n>> Tuning {name} ...\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,\n",
    "        scoring={\"acc\": \"accuracy\", \"rec3\": recall3_scorer},\n",
    "        refit=\"rec3\",\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    best = search.best_estimator_\n",
    "    test_acc  = accuracy_score(y_test, best.predict(X_test))\n",
    "    test_rec3 = recall_at_3(y_test, best.predict_proba(X_test))\n",
    "\n",
    "    best_results.append({\n",
    "        \"Model\": name,\n",
    "        \"BestParams\": search.best_params_,\n",
    "        \"Val_Recall@3\": f\"{search.best_score_:.2%}\",\n",
    "        \"Test_Acc\": f\"{test_acc:.2%}\",\n",
    "        \"Test_Recall@3\": f\"{test_rec3:.2%}\"\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# 9. 列印結果\n",
    "# -----------------------------\n",
    "results_df = pd.DataFrame(best_results)\n",
    "print(\"\\n=== Final Comparison ===\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f460a7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BestParams</th>\n",
       "      <th>Val_Recall@3</th>\n",
       "      <th>Test_Acc</th>\n",
       "      <th>Test_Recall@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 0.21584494295802448, 'penalty': 'l2', 's...</td>\n",
       "      <td>39.89%</td>\n",
       "      <td>13.66%</td>\n",
       "      <td>40.02%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{'max_depth': 6, 'max_features': None, 'n_esti...</td>\n",
       "      <td>51.17%</td>\n",
       "      <td>20.55%</td>\n",
       "      <td>53.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HistGradientBoost</td>\n",
       "      <td>{'learning_rate': 0.010233629752304298, 'max_i...</td>\n",
       "      <td>48.19%</td>\n",
       "      <td>18.69%</td>\n",
       "      <td>50.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>{'max_depth': 6, 'max_features': None, 'n_esti...</td>\n",
       "      <td>52.37%</td>\n",
       "      <td>20.59%</td>\n",
       "      <td>53.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'learning_rate': 0.010233629752304298, 'n_est...</td>\n",
       "      <td>48.24%</td>\n",
       "      <td>18.05%</td>\n",
       "      <td>50.36%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model                                         BestParams  \\\n",
       "0  LogisticRegression  {'C': 0.21584494295802448, 'penalty': 'l2', 's...   \n",
       "1        RandomForest  {'max_depth': 6, 'max_features': None, 'n_esti...   \n",
       "2   HistGradientBoost  {'learning_rate': 0.010233629752304298, 'max_i...   \n",
       "3          ExtraTrees  {'max_depth': 6, 'max_features': None, 'n_esti...   \n",
       "4            LightGBM  {'learning_rate': 0.010233629752304298, 'n_est...   \n",
       "\n",
       "  Val_Recall@3 Test_Acc Test_Recall@3  \n",
       "0       39.89%   13.66%        40.02%  \n",
       "1       51.17%   20.55%        53.52%  \n",
       "2       48.19%   18.69%        50.36%  \n",
       "3       52.37%   20.59%        53.92%  \n",
       "4       48.24%   18.05%        50.36%  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca4175e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1303\n",
      "[LightGBM] [Info] Number of data points in the train set: 40004, number of used features: 34\n",
      "[10]\tvalid_0's ndcg@3: 0.106144\n",
      "[20]\tvalid_0's ndcg@3: 0\n",
      "[30]\tvalid_0's ndcg@3: 0.0469279\n",
      "[40]\tvalid_0's ndcg@3: 0\n",
      "[50]\tvalid_0's ndcg@3: 0\n",
      "[60]\tvalid_0's ndcg@3: 0.0592164\n",
      "[70]\tvalid_0's ndcg@3: 0.0592164\n",
      "[80]\tvalid_0's ndcg@3: 0\n",
      "[90]\tvalid_0's ndcg@3: 0\n",
      "[100]\tvalid_0's ndcg@3: 0.0469279\n",
      "[110]\tvalid_0's ndcg@3: 0.0469279\n",
      "[120]\tvalid_0's ndcg@3: 0\n",
      "[130]\tvalid_0's ndcg@3: 0.0592164\n",
      "[140]\tvalid_0's ndcg@3: 0.106144\n",
      "[150]\tvalid_0's ndcg@3: 0.106144\n",
      "[160]\tvalid_0's ndcg@3: 0.106144\n",
      "[170]\tvalid_0's ndcg@3: 0.106144\n",
      "[180]\tvalid_0's ndcg@3: 0.0592164\n",
      "[190]\tvalid_0's ndcg@3: 0.106144\n",
      "[200]\tvalid_0's ndcg@3: 0.153072\n",
      "[210]\tvalid_0's ndcg@3: 0.153072\n",
      "[220]\tvalid_0's ndcg@3: 0.2\n",
      "[230]\tvalid_0's ndcg@3: 0.224577\n",
      "[240]\tvalid_0's ndcg@3: 0.259216\n",
      "[250]\tvalid_0's ndcg@3: 0.259216\n",
      "[260]\tvalid_0's ndcg@3: 0.259216\n",
      "[270]\tvalid_0's ndcg@3: 0.259216\n",
      "[280]\tvalid_0's ndcg@3: 0.259216\n",
      "[290]\tvalid_0's ndcg@3: 0.224577\n",
      "[300]\tvalid_0's ndcg@3: 0.271505\n",
      "[310]\tvalid_0's ndcg@3: 0.224577\n",
      "[320]\tvalid_0's ndcg@3: 0.224577\n",
      "[330]\tvalid_0's ndcg@3: 0.212289\n",
      "[340]\tvalid_0's ndcg@3: 0.224577\n",
      "[350]\tvalid_0's ndcg@3: 0.259216\n",
      "[360]\tvalid_0's ndcg@3: 0.246928\n",
      "[370]\tvalid_0's ndcg@3: 0.259216\n",
      "[380]\tvalid_0's ndcg@3: 0.259216\n",
      "[390]\tvalid_0's ndcg@3: 0.246928\n",
      "[400]\tvalid_0's ndcg@3: 0.246928\n",
      "[410]\tvalid_0's ndcg@3: 0.246928\n",
      "[420]\tvalid_0's ndcg@3: 0.246928\n",
      "[430]\tvalid_0's ndcg@3: 0.246928\n",
      "[440]\tvalid_0's ndcg@3: 0.246928\n",
      "[450]\tvalid_0's ndcg@3: 0.246928\n",
      "[460]\tvalid_0's ndcg@3: 0.246928\n",
      "[470]\tvalid_0's ndcg@3: 0.246928\n",
      "[480]\tvalid_0's ndcg@3: 0.306144\n",
      "[490]\tvalid_0's ndcg@3: 0.306144\n",
      "[500]\tvalid_0's ndcg@3: 0.306144\n",
      "[510]\tvalid_0's ndcg@3: 0.306144\n",
      "[520]\tvalid_0's ndcg@3: 0.306144\n",
      "[530]\tvalid_0's ndcg@3: 0.306144\n",
      "[540]\tvalid_0's ndcg@3: 0.340784\n",
      "[550]\tvalid_0's ndcg@3: 0.340784\n",
      "[560]\tvalid_0's ndcg@3: 0.306144\n",
      "[570]\tvalid_0's ndcg@3: 0.306144\n",
      "[580]\tvalid_0's ndcg@3: 0.306144\n",
      "[590]\tvalid_0's ndcg@3: 0.306144\n",
      "[600]\tvalid_0's ndcg@3: 0.306144\n",
      "[610]\tvalid_0's ndcg@3: 0.306144\n",
      "[620]\tvalid_0's ndcg@3: 0.306144\n",
      "[630]\tvalid_0's ndcg@3: 0.306144\n",
      "[640]\tvalid_0's ndcg@3: 0.306144\n",
      "[650]\tvalid_0's ndcg@3: 0.306144\n",
      "[660]\tvalid_0's ndcg@3: 0.306144\n",
      "[670]\tvalid_0's ndcg@3: 0.306144\n",
      "[680]\tvalid_0's ndcg@3: 0.306144\n",
      "[690]\tvalid_0's ndcg@3: 0.306144\n",
      "[700]\tvalid_0's ndcg@3: 0.306144\n",
      "[710]\tvalid_0's ndcg@3: 0.306144\n",
      "[720]\tvalid_0's ndcg@3: 0.306144\n",
      "[730]\tvalid_0's ndcg@3: 0.306144\n",
      "[740]\tvalid_0's ndcg@3: 0.353072\n",
      "[750]\tvalid_0's ndcg@3: 0.4\n",
      "[760]\tvalid_0's ndcg@3: 0.4\n",
      "[770]\tvalid_0's ndcg@3: 0.4\n",
      "[780]\tvalid_0's ndcg@3: 0.4\n",
      "[790]\tvalid_0's ndcg@3: 0.4\n",
      "[800]\tvalid_0's ndcg@3: 0.4\n",
      "[810]\tvalid_0's ndcg@3: 0.4\n",
      "[820]\tvalid_0's ndcg@3: 0.446928\n",
      "[830]\tvalid_0's ndcg@3: 0.446928\n",
      "[840]\tvalid_0's ndcg@3: 0.481567\n",
      "[850]\tvalid_0's ndcg@3: 0.481567\n",
      "[860]\tvalid_0's ndcg@3: 0.481567\n",
      "[870]\tvalid_0's ndcg@3: 0.481567\n",
      "[880]\tvalid_0's ndcg@3: 0.481567\n",
      "[890]\tvalid_0's ndcg@3: 0.481567\n",
      "[900]\tvalid_0's ndcg@3: 0.481567\n",
      "[910]\tvalid_0's ndcg@3: 0.481567\n",
      "[920]\tvalid_0's ndcg@3: 0.481567\n",
      "[930]\tvalid_0's ndcg@3: 0.434639\n",
      "[940]\tvalid_0's ndcg@3: 0.434639\n",
      "[950]\tvalid_0's ndcg@3: 0.434639\n",
      "[960]\tvalid_0's ndcg@3: 0.481567\n",
      "[970]\tvalid_0's ndcg@3: 0.481567\n",
      "[980]\tvalid_0's ndcg@3: 0.481567\n",
      "[990]\tvalid_0's ndcg@3: 0.481567\n",
      "[1000]\tvalid_0's ndcg@3: 0.481567\n",
      "Ranker Test Top-3 Recall:  0.8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.metrics import ndcg_score, recall_score\n",
    "from lightgbm import LGBMRanker, log_evaluation, early_stopping\n",
    "# 1. 读取并预处理（跟你之前的 pipeline 一致，只留关键步骤）\n",
    "df = pd.read_csv(\"retail_store_sales_cleaned_feature_engineering.csv\",\n",
    "                 parse_dates=[\"Transaction Date\"])\n",
    "df = df.sort_values([\"Customer ID\", \"Transaction Date\"]).reset_index(drop=True)\n",
    "\n",
    "# 假设你已经做完所有特征工程，并把 RFM、多窗口特征、聚类特征都加进来了：\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"Category_ID\"] = le.fit_transform(df[\"Category\"])\n",
    "# -----------------------------\n",
    "# 3. 衍生特徵：Inter_Days、30天RFM、週期編碼、flags\n",
    "# -----------------------------\n",
    "df[\"Prev_Date\"] = df.groupby(\"Customer ID\")[\"Transaction Date\"].shift(1)\n",
    "df[\"Inter_Days\"] = (df[\"Transaction Date\"] - df[\"Prev_Date\"])\\\n",
    "                   .dt.days.fillna(0)\n",
    "\n",
    "window_times, window_amounts = deque(), deque()\n",
    "freq30 = np.zeros(len(df), int)\n",
    "amt30_sum = np.zeros(len(df))\n",
    "amt30_count = np.zeros(len(df))\n",
    "\n",
    "for i, (cust, t, amt) in enumerate(zip(\n",
    "    df[\"Customer ID\"],\n",
    "    df[\"Transaction Date\"],\n",
    "    df[\"Total Spent\"]\n",
    ")):\n",
    "    if i == 0 or cust != df.loc[i-1, \"Customer ID\"]:\n",
    "        window_times.clear(); window_amounts.clear()\n",
    "    window_times.append(t); window_amounts.append(amt)\n",
    "    while (t - window_times[0]).days > 30:\n",
    "        window_times.popleft(); window_amounts.popleft()\n",
    "    freq30[i]     = len(window_times) - 1\n",
    "    amt30_sum[i]  = sum(window_amounts)\n",
    "    amt30_count[i]= len(window_times)\n",
    "\n",
    "df[\"freq30\"]     = freq30\n",
    "df[\"amt30_mean\"] = amt30_sum / amt30_count\n",
    "\n",
    "# 週期編碼\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"day_sin\"]   = np.sin(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "df[\"day_cos\"]   = np.cos(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "\n",
    "bool_cols = [\n",
    "    \"Is_Weekend\",\"Is_Holiday\",\"Is_NonWorkday\",\n",
    "    \"PM_Credit Card\",\"PM_Digital Wallet\",\n",
    "    \"Loc_Online\",\"Disc_True\",\"Disc_Unknown\"\n",
    "]\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Price Per Unit\",\"Quantity\",\"Total Spent\",\"Recency_Cust\",\n",
    "    \"Inter_Days\",\"freq30\",\"amt30_mean\",\n",
    "    \"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"\n",
    "] + bool_cols\n",
    "\n",
    "def rolling_rfm(df, window_days, amt_col=\"Total Spent\"):\n",
    "    times, amts, freqs, sums = deque(), deque(), [], []\n",
    "    for cust, group in df.groupby(\"Customer ID\", sort=False):\n",
    "        times.clear(); amts.clear()\n",
    "        for t, amt in zip(group[\"Transaction Date\"], group[amt_col]):\n",
    "            times.append(t); amts.append(amt)\n",
    "            # pop 超出 window_days\n",
    "            while (t - times[0]).days > window_days:\n",
    "                times.popleft(); amts.popleft()\n",
    "            freqs.append(len(times)-1)            # 扣掉自己\n",
    "            sums.append(sum(amts))\n",
    "    return freqs, sums\n",
    "\n",
    "df = df.sort_values([\"Customer ID\",\"Transaction Date\"]).reset_index(drop=True)\n",
    "# 7 天\n",
    "df[\"freq7\"],  df[\"amt7_sum\"]  = rolling_rfm(df,  7)\n",
    "# 30 天（以前已有 freq30, amt30_mean），如果想保留 sum 可覆蓋或改名\n",
    "# 90 天\n",
    "df[\"freq90\"], df[\"amt90_sum\"] = rolling_rfm(df, 90)\n",
    "\n",
    "# 為了跟 amt30_mean 同步，上面 sum 改成 mean 也很簡單：\n",
    "df[\"amt7_mean\"]  = df[\"amt7_sum\"]  / (df[\"freq7\"]  + 1)  # +1 包含當前交易\n",
    "df[\"amt90_mean\"] = df[\"amt90_sum\"] / (df[\"freq90\"] + 1)\n",
    "\n",
    "# --- 2. 距今最後購買日的指數衰減特徵（Recency） ---------------------\n",
    "\n",
    "# 先算「距今天數」\n",
    "today = df[\"Transaction Date\"].max()  # or datetime.today()\n",
    "last_purchase = df.groupby(\"Customer ID\")[\"Transaction Date\"].transform(\"max\")\n",
    "df[\"days_since_last\"] = (today - last_purchase).dt.days\n",
    "\n",
    "# 再做指數衰減：exp(-λ·days)，λ 可調\n",
    "lam = 0.05\n",
    "df[\"recency_exp\"] = np.exp(-lam * df[\"days_since_last\"])\n",
    "\n",
    "# --- 3. 客戶整體 RFM 聚類 -----------------------------------------\n",
    "\n",
    "# 為每位客戶計算整體 RFM 向量\n",
    "rfm = df.groupby(\"Customer ID\").agg({\n",
    "    \"days_since_last\": \"min\",               # Recency：最近那筆交易距今\n",
    "    \"Transaction ID\": \"count\",              # Frequency：交易次數\n",
    "    \"Total Spent\": \"sum\"                    # Monetary：總花費\n",
    "}).rename(columns={\n",
    "    \"days_since_last\": \"R\",\n",
    "    \"Transaction ID\": \"F\",\n",
    "    \"Total Spent\": \"M\"\n",
    "})\n",
    "\n",
    "# 標準化後做 KMeans\n",
    "scaler_rfm = StandardScaler()\n",
    "rfm_scaled = scaler_rfm.fit_transform(rfm[[\"R\",\"F\",\"M\"]])\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "rfm[\"cluster\"] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Merge 回原始 df\n",
    "df = df.merge(rfm[\"cluster\"], left_on=\"Customer ID\", right_index=True)\n",
    "\n",
    "# --- 4. 更新 feature_cols ---------------------------------------\n",
    "\n",
    "new_feats = [\n",
    "    \"freq7\", \"amt7_mean\",\n",
    "    \"freq90\", \"amt90_mean\",\n",
    "    \"days_since_last\", \"recency_exp\",\n",
    "    \"cluster\"\n",
    "]\n",
    "feature_cols += new_feats\n",
    "# 2. 为每个“查询”（query），即每一次要预测下一品类的历史序列，构造正负样本：\n",
    "#    把每条正样本 (seq → true next Category_ID) 和随机采的几个负样本拼到一起，用同一个 query_id 分组。\n",
    "records = []\n",
    "for cust, grp in df.groupby(\"Customer ID\"):\n",
    "    cats = grp[\"Category_ID\"].values\n",
    "    feats = grp[feature_cols].values\n",
    "    for i in range(1, len(grp)):\n",
    "        hist_feat = feats[i-1]  # 这里用“上一笔”特征，也可以用窗口聚合等\n",
    "        true_cat  = cats[i]\n",
    "        # 正样本\n",
    "        records.append((cust, hist_feat, true_cat, 1))\n",
    "        # 负采样 3 个\n",
    "        for _ in range(3):\n",
    "            neg = np.random.randint(0, df[\"Category_ID\"].nunique())\n",
    "            while neg == true_cat:\n",
    "                neg = np.random.randint(0, df[\"Category_ID\"].nunique())\n",
    "            records.append((cust, hist_feat, neg, 0))\n",
    "\n",
    "rank_df = pd.DataFrame(records, columns=[\"query_id\",\"feat\",\"cat_id\",\"label\"])\n",
    "\n",
    "# 3. 构造训练矩阵：将 feat 展开、cat_id 用 one-hot 或 embedding 特征拼进去\n",
    "#    这里示例：直接把 feat + “cat_id” 做 one-hot（也可用 embedding lookup）。\n",
    "X = np.stack(rank_df[\"feat\"].values)\n",
    "# one-hot encode cat_id：\n",
    "K = df[\"Category_ID\"].nunique()\n",
    "onehots = np.eye(K)[rank_df[\"cat_id\"].values]\n",
    "X = np.hstack([X, onehots])\n",
    "\n",
    "y = rank_df[\"label\"].values\n",
    "# group sizes = 每个 query_id 下的样本数量\n",
    "group = rank_df.groupby(\"query_id\").size().astype(int).tolist()\n",
    "\n",
    "# 4. 切分（保证 query 整体不拆分）\n",
    "qids = rank_df[\"query_id\"].unique()\n",
    "q_train, q_test = train_test_split(qids, test_size=0.2, random_state=42)\n",
    "train_mask = rank_df[\"query_id\"].isin(q_train)\n",
    "X_train, y_train, g_train = X[train_mask], y[train_mask], rank_df[train_mask]\\\n",
    "    .groupby(\"query_id\")\\\n",
    "    .size()\\\n",
    "    .astype(int)\\\n",
    "    .tolist()\n",
    "X_test, y_test, g_test = X[~train_mask], y[~train_mask], rank_df[~train_mask]\\\n",
    "    .groupby(\"query_id\")\\\n",
    "    .size()\\\n",
    "    .astype(int)\\\n",
    "    .tolist()\n",
    "\n",
    "# 5. 用 LGBMRanker 训练\n",
    "callbacks = [\n",
    "    log_evaluation(period=10),\n",
    "\n",
    "]\n",
    "\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    random_state=42,\n",
    "    importance_type=\"gain\"\n",
    ")\n",
    "\n",
    "ranker.fit(\n",
    "    X_train, y_train,\n",
    "    group=g_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_group=[g_test],\n",
    "    eval_at=[3],          # NDCG@3\n",
    "    callbacks=callbacks    # ← 在这里传入 callbacks，取代 verbose\n",
    ")\n",
    "# 6. 评估 Top-3 Recall\n",
    "#    对测试每个 query，predict 得分后取 top3，看正样本是否在内\n",
    "pred_scores = ranker.predict(X_test)\n",
    "# 重组为 per-query scores\n",
    "offset = 0\n",
    "hits = []\n",
    "for grp_size in g_test:\n",
    "    grp_scores = pred_scores[offset:offset+grp_size]\n",
    "    grp_labels = y_test      [offset:offset+grp_size]\n",
    "    topk_idx   = np.argsort(grp_scores)[-3:]\n",
    "    # 只要正样本(label=1)在 top3 就算命中\n",
    "    hits.append(any(grp_labels[i]==1 for i in topk_idx))\n",
    "    offset += grp_size\n",
    "\n",
    "print(\"Ranker Test Top-3 Recall: \", np.mean(hits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8654eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1305\n",
      "[LightGBM] [Info] Number of data points in the train set: 80320, number of used features: 34\n",
      "[10]\tvalid_0's ndcg@3: 0.274812\n",
      "[20]\tvalid_0's ndcg@3: 0.276216\n",
      "[30]\tvalid_0's ndcg@3: 0.276363\n",
      "[40]\tvalid_0's ndcg@3: 0.279313\n",
      "[50]\tvalid_0's ndcg@3: 0.272303\n",
      "[60]\tvalid_0's ndcg@3: 0.27135\n",
      "[70]\tvalid_0's ndcg@3: 0.271317\n",
      "[80]\tvalid_0's ndcg@3: 0.276088\n",
      "[90]\tvalid_0's ndcg@3: 0.272052\n",
      "[100]\tvalid_0's ndcg@3: 0.271577\n",
      "[110]\tvalid_0's ndcg@3: 0.2718\n",
      "[120]\tvalid_0's ndcg@3: 0.270657\n",
      "[130]\tvalid_0's ndcg@3: 0.268855\n",
      "[140]\tvalid_0's ndcg@3: 0.265246\n",
      "[150]\tvalid_0's ndcg@3: 0.264112\n",
      "[160]\tvalid_0's ndcg@3: 0.265046\n",
      "[170]\tvalid_0's ndcg@3: 0.266398\n",
      "[180]\tvalid_0's ndcg@3: 0.266588\n",
      "[190]\tvalid_0's ndcg@3: 0.267029\n",
      "[200]\tvalid_0's ndcg@3: 0.267982\n",
      "[210]\tvalid_0's ndcg@3: 0.269021\n",
      "[220]\tvalid_0's ndcg@3: 0.269514\n",
      "[230]\tvalid_0's ndcg@3: 0.266882\n",
      "[240]\tvalid_0's ndcg@3: 0.268286\n",
      "[250]\tvalid_0's ndcg@3: 0.268822\n",
      "[260]\tvalid_0's ndcg@3: 0.270804\n",
      "[270]\tvalid_0's ndcg@3: 0.270382\n",
      "[280]\tvalid_0's ndcg@3: 0.270781\n",
      "[290]\tvalid_0's ndcg@3: 0.269405\n",
      "[300]\tvalid_0's ndcg@3: 0.270937\n",
      "[310]\tvalid_0's ndcg@3: 0.271933\n",
      "[320]\tvalid_0's ndcg@3: 0.273138\n",
      "[330]\tvalid_0's ndcg@3: 0.273788\n",
      "[340]\tvalid_0's ndcg@3: 0.274523\n",
      "[350]\tvalid_0's ndcg@3: 0.276852\n",
      "[360]\tvalid_0's ndcg@3: 0.275267\n",
      "[370]\tvalid_0's ndcg@3: 0.275078\n",
      "[380]\tvalid_0's ndcg@3: 0.273769\n",
      "[390]\tvalid_0's ndcg@3: 0.27513\n",
      "[400]\tvalid_0's ndcg@3: 0.276211\n",
      "[410]\tvalid_0's ndcg@3: 0.272626\n",
      "[420]\tvalid_0's ndcg@3: 0.270401\n",
      "[430]\tvalid_0's ndcg@3: 0.271041\n",
      "[440]\tvalid_0's ndcg@3: 0.272289\n",
      "[450]\tvalid_0's ndcg@3: 0.271231\n",
      "[460]\tvalid_0's ndcg@3: 0.271672\n",
      "[470]\tvalid_0's ndcg@3: 0.270615\n",
      "[480]\tvalid_0's ndcg@3: 0.269325\n",
      "[490]\tvalid_0's ndcg@3: 0.27162\n",
      "[500]\tvalid_0's ndcg@3: 0.272208\n",
      "[510]\tvalid_0's ndcg@3: 0.268623\n",
      "[520]\tvalid_0's ndcg@3: 0.270563\n",
      "[530]\tvalid_0's ndcg@3: 0.269116\n",
      "[540]\tvalid_0's ndcg@3: 0.269619\n",
      "[550]\tvalid_0's ndcg@3: 0.270918\n",
      "[560]\tvalid_0's ndcg@3: 0.271412\n",
      "[570]\tvalid_0's ndcg@3: 0.270183\n",
      "[580]\tvalid_0's ndcg@3: 0.27208\n",
      "[590]\tvalid_0's ndcg@3: 0.273171\n",
      "[600]\tvalid_0's ndcg@3: 0.273015\n",
      "[610]\tvalid_0's ndcg@3: 0.273456\n",
      "[620]\tvalid_0's ndcg@3: 0.274087\n",
      "[630]\tvalid_0's ndcg@3: 0.274599\n",
      "[640]\tvalid_0's ndcg@3: 0.274305\n",
      "[650]\tvalid_0's ndcg@3: 0.274409\n",
      "[660]\tvalid_0's ndcg@3: 0.274736\n",
      "[670]\tvalid_0's ndcg@3: 0.273887\n",
      "[680]\tvalid_0's ndcg@3: 0.27374\n",
      "[690]\tvalid_0's ndcg@3: 0.274286\n",
      "[700]\tvalid_0's ndcg@3: 0.275083\n",
      "[710]\tvalid_0's ndcg@3: 0.27439\n",
      "[720]\tvalid_0's ndcg@3: 0.273698\n",
      "[730]\tvalid_0's ndcg@3: 0.273992\n",
      "[740]\tvalid_0's ndcg@3: 0.271853\n",
      "[750]\tvalid_0's ndcg@3: 0.27116\n",
      "[760]\tvalid_0's ndcg@3: 0.271099\n",
      "[770]\tvalid_0's ndcg@3: 0.270207\n",
      "[780]\tvalid_0's ndcg@3: 0.27005\n",
      "[790]\tvalid_0's ndcg@3: 0.270197\n",
      "[800]\tvalid_0's ndcg@3: 0.272242\n",
      "[810]\tvalid_0's ndcg@3: 0.273645\n",
      "[820]\tvalid_0's ndcg@3: 0.272408\n",
      "[830]\tvalid_0's ndcg@3: 0.271317\n",
      "[840]\tvalid_0's ndcg@3: 0.270942\n",
      "[850]\tvalid_0's ndcg@3: 0.271246\n",
      "[860]\tvalid_0's ndcg@3: 0.270207\n",
      "[870]\tvalid_0's ndcg@3: 0.272303\n",
      "[880]\tvalid_0's ndcg@3: 0.273385\n",
      "[890]\tvalid_0's ndcg@3: 0.273551\n",
      "[900]\tvalid_0's ndcg@3: 0.273783\n",
      "[910]\tvalid_0's ndcg@3: 0.272796\n",
      "[920]\tvalid_0's ndcg@3: 0.271966\n",
      "[930]\tvalid_0's ndcg@3: 0.274011\n",
      "[940]\tvalid_0's ndcg@3: 0.274547\n",
      "[950]\tvalid_0's ndcg@3: 0.273309\n",
      "[960]\tvalid_0's ndcg@3: 0.27466\n",
      "[970]\tvalid_0's ndcg@3: 0.274556\n",
      "[980]\tvalid_0's ndcg@3: 0.273404\n",
      "[990]\tvalid_0's ndcg@3: 0.272711\n",
      "[1000]\tvalid_0's ndcg@3: 0.272616\n",
      "Ranker Test Top-3 Recall:  0.38207171314741034\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.metrics import ndcg_score, recall_score\n",
    "from lightgbm import LGBMRanker, log_evaluation, early_stopping\n",
    "# 1. 读取并预处理（跟你之前的 pipeline 一致，只留关键步骤）\n",
    "df = pd.read_csv(\"retail_store_sales_cleaned_feature_engineering.csv\",\n",
    "                 parse_dates=[\"Transaction Date\"])\n",
    "df = df.sort_values([\"Customer ID\", \"Transaction Date\"]).reset_index(drop=True)\n",
    "\n",
    "# 假设你已经做完所有特征工程，并把 RFM、多窗口特征、聚类特征都加进来了：\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"Category_ID\"] = le.fit_transform(df[\"Category\"])\n",
    "# -----------------------------\n",
    "# 3. 衍生特徵：Inter_Days、30天RFM、週期編碼、flags\n",
    "# -----------------------------\n",
    "df[\"Prev_Date\"] = df.groupby(\"Customer ID\")[\"Transaction Date\"].shift(1)\n",
    "df[\"Inter_Days\"] = (df[\"Transaction Date\"] - df[\"Prev_Date\"])\\\n",
    "                   .dt.days.fillna(0)\n",
    "\n",
    "window_times, window_amounts = deque(), deque()\n",
    "freq30 = np.zeros(len(df), int)\n",
    "amt30_sum = np.zeros(len(df))\n",
    "amt30_count = np.zeros(len(df))\n",
    "\n",
    "for i, (cust, t, amt) in enumerate(zip(\n",
    "    df[\"Customer ID\"],\n",
    "    df[\"Transaction Date\"],\n",
    "    df[\"Total Spent\"]\n",
    ")):\n",
    "    if i == 0 or cust != df.loc[i-1, \"Customer ID\"]:\n",
    "        window_times.clear(); window_amounts.clear()\n",
    "    window_times.append(t); window_amounts.append(amt)\n",
    "    while (t - window_times[0]).days > 30:\n",
    "        window_times.popleft(); window_amounts.popleft()\n",
    "    freq30[i]     = len(window_times) - 1\n",
    "    amt30_sum[i]  = sum(window_amounts)\n",
    "    amt30_count[i]= len(window_times)\n",
    "\n",
    "df[\"freq30\"]     = freq30\n",
    "df[\"amt30_mean\"] = amt30_sum / amt30_count\n",
    "\n",
    "# 週期編碼\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"Month\"]  / 12)\n",
    "df[\"day_sin\"]   = np.sin(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "df[\"day_cos\"]   = np.cos(2*np.pi*(df[\"Day\"]-1)/31)\n",
    "\n",
    "bool_cols = [\n",
    "    \"Is_Weekend\",\"Is_Holiday\",\"Is_NonWorkday\",\n",
    "    \"PM_Credit Card\",\"PM_Digital Wallet\",\n",
    "    \"Loc_Online\",\"Disc_True\",\"Disc_Unknown\"\n",
    "]\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    \"Price Per Unit\",\"Quantity\",\"Total Spent\",\"Recency_Cust\",\n",
    "    \"Inter_Days\",\"freq30\",\"amt30_mean\",\n",
    "    \"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\"\n",
    "] + bool_cols\n",
    "\n",
    "def rolling_rfm(df, window_days, amt_col=\"Total Spent\"):\n",
    "    times, amts, freqs, sums = deque(), deque(), [], []\n",
    "    for cust, group in df.groupby(\"Customer ID\", sort=False):\n",
    "        times.clear(); amts.clear()\n",
    "        for t, amt in zip(group[\"Transaction Date\"], group[amt_col]):\n",
    "            times.append(t); amts.append(amt)\n",
    "            # pop 超出 window_days\n",
    "            while (t - times[0]).days > window_days:\n",
    "                times.popleft(); amts.popleft()\n",
    "            freqs.append(len(times)-1)            # 扣掉自己\n",
    "            sums.append(sum(amts))\n",
    "    return freqs, sums\n",
    "\n",
    "df = df.sort_values([\"Customer ID\",\"Transaction Date\"]).reset_index(drop=True)\n",
    "# 7 天\n",
    "df[\"freq7\"],  df[\"amt7_sum\"]  = rolling_rfm(df,  7)\n",
    "# 30 天（以前已有 freq30, amt30_mean），如果想保留 sum 可覆蓋或改名\n",
    "# 90 天\n",
    "df[\"freq90\"], df[\"amt90_sum\"] = rolling_rfm(df, 90)\n",
    "\n",
    "# 為了跟 amt30_mean 同步，上面 sum 改成 mean 也很簡單：\n",
    "df[\"amt7_mean\"]  = df[\"amt7_sum\"]  / (df[\"freq7\"]  + 1)  # +1 包含當前交易\n",
    "df[\"amt90_mean\"] = df[\"amt90_sum\"] / (df[\"freq90\"] + 1)\n",
    "\n",
    "# --- 2. 距今最後購買日的指數衰減特徵（Recency） ---------------------\n",
    "\n",
    "# 先算「距今天數」\n",
    "today = df[\"Transaction Date\"].max()  # or datetime.today()\n",
    "last_purchase = df.groupby(\"Customer ID\")[\"Transaction Date\"].transform(\"max\")\n",
    "df[\"days_since_last\"] = (today - last_purchase).dt.days\n",
    "\n",
    "# 再做指數衰減：exp(-λ·days)，λ 可調\n",
    "lam = 0.05\n",
    "df[\"recency_exp\"] = np.exp(-lam * df[\"days_since_last\"])\n",
    "\n",
    "# --- 3. 客戶整體 RFM 聚類 -----------------------------------------\n",
    "\n",
    "# 為每位客戶計算整體 RFM 向量\n",
    "rfm = df.groupby(\"Customer ID\").agg({\n",
    "    \"days_since_last\": \"min\",               # Recency：最近那筆交易距今\n",
    "    \"Transaction ID\": \"count\",              # Frequency：交易次數\n",
    "    \"Total Spent\": \"sum\"                    # Monetary：總花費\n",
    "}).rename(columns={\n",
    "    \"days_since_last\": \"R\",\n",
    "    \"Transaction ID\": \"F\",\n",
    "    \"Total Spent\": \"M\"\n",
    "})\n",
    "\n",
    "# 標準化後做 KMeans\n",
    "scaler_rfm = StandardScaler()\n",
    "rfm_scaled = scaler_rfm.fit_transform(rfm[[\"R\",\"F\",\"M\"]])\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "rfm[\"cluster\"] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Merge 回原始 df\n",
    "df = df.merge(rfm[\"cluster\"], left_on=\"Customer ID\", right_index=True)\n",
    "\n",
    "# --- 4. 更新 feature_cols ---------------------------------------\n",
    "\n",
    "new_feats = [\n",
    "    \"freq7\", \"amt7_mean\",\n",
    "    \"freq90\", \"amt90_mean\",\n",
    "    \"days_since_last\", \"recency_exp\",\n",
    "    \"cluster\"\n",
    "]\n",
    "feature_cols += new_feats\n",
    "# 2. 为每个“查询”（query），即每一次要预测下一品类的历史序列，构造正负样本：\n",
    "#    把每条正样本 (seq → true next Category_ID) 和随机采的几个负样本拼到一起，用同一个 query_id 分组。\n",
    "records = []\n",
    "for cust, grp in df.groupby(\"Customer ID\"):\n",
    "    cats = grp[\"Category_ID\"].values\n",
    "    feats = grp[feature_cols].values\n",
    "    for i in range(1, len(grp)):\n",
    "        hist_feat = feats[i-1]  # 这里用“上一笔”特征，也可以用窗口聚合等\n",
    "        true_cat  = cats[i]\n",
    "        # 正样本\n",
    "        records.append((cust, hist_feat, true_cat, 1))\n",
    "        # 负采样 3 个\n",
    "        for _ in range(3):\n",
    "            neg = np.random.randint(0, df[\"Category_ID\"].nunique())\n",
    "            while neg == true_cat:\n",
    "                neg = np.random.randint(0, df[\"Category_ID\"].nunique())\n",
    "            records.append((cust, hist_feat, neg, 0))\n",
    "\n",
    "rank_df = pd.DataFrame(records, columns=[\"query_id\",\"feat\",\"cat_id\",\"label\"])\n",
    "\n",
    "# 3. 构造训练矩阵：将 feat 展开、cat_id 用 one-hot 或 embedding 特征拼进去\n",
    "#    这里示例：直接把 feat + “cat_id” 做 one-hot（也可用 embedding lookup）。\n",
    "X = np.stack(rank_df[\"feat\"].values)\n",
    "# one-hot encode cat_id：\n",
    "\n",
    "# K: 總類別數\n",
    "K = df[\"Category_ID\"].nunique()\n",
    "\n",
    "records = []\n",
    "query_counter = 0\n",
    "\n",
    "for cust, grp in df.groupby(\"Customer ID\", sort=False):\n",
    "    feats = grp[feature_cols].values\n",
    "    cats  = grp[\"Category_ID\"].values\n",
    "    for i in range(1, len(grp)):\n",
    "        hist_feat = feats[i-1]\n",
    "        true_cat  = cats[i]\n",
    "        # 這筆歷史序列就用 query_counter 當作唯一 ID\n",
    "        for cat_id in range(K):\n",
    "            label = 1 if cat_id == true_cat else 0\n",
    "            records.append((query_counter, hist_feat, cat_id, label))\n",
    "        query_counter += 1\n",
    "\n",
    "rank_df = pd.DataFrame(records,\n",
    "    columns=[\"query_id\",\"feat\",\"cat_id\",\"label\"])\n",
    "\n",
    "# 展開特徵\n",
    "X = np.vstack(rank_df[\"feat\"].values)\n",
    "onehots = np.eye(K)[rank_df[\"cat_id\"].values]\n",
    "X = np.hstack([X, onehots])\n",
    "y = rank_df[\"label\"].values\n",
    "\n",
    "# 正確地用 query_id 分組，得到每筆 query 的樣本數（應該都等於 K）\n",
    "group_sizes = rank_df.groupby(\"query_id\").size().tolist()  \n",
    "\n",
    "# 切分 query_ids\n",
    "all_qids = np.arange(query_counter)\n",
    "q_train, q_test = train_test_split(all_qids,\n",
    "    test_size=0.2, random_state=42)\n",
    "\n",
    "# 建立訓練／測試遮罩\n",
    "train_mask = rank_df[\"query_id\"].isin(q_train)\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "g_train = [K] * len(q_train)\n",
    "\n",
    "X_test = X[~train_mask]\n",
    "y_test = y[~train_mask]\n",
    "g_test = [K] * len(q_test)\n",
    "# 5. 用 LGBMRanker 训练\n",
    "callbacks = [\n",
    "    log_evaluation(period=10),\n",
    "\n",
    "]\n",
    "\n",
    "ranker = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    random_state=42,\n",
    "    importance_type=\"gain\"\n",
    ")\n",
    "\n",
    "ranker.fit(\n",
    "    X_train, y_train,\n",
    "    group=g_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_group=[g_test],\n",
    "    eval_at=[3],          # NDCG@3\n",
    "    callbacks=callbacks    # ← 在这里传入 callbacks，取代 verbose\n",
    ")\n",
    "# 6. 评估 Top-3 Recall\n",
    "#    对测试每个 query，predict 得分后取 top3，看正样本是否在内\n",
    "pred_scores = ranker.predict(X_test)\n",
    "# 重组为 per-query scores\n",
    "offset = 0\n",
    "hits = []\n",
    "for grp_size in g_test:\n",
    "    grp_scores = pred_scores[offset:offset+grp_size]\n",
    "    grp_labels = y_test      [offset:offset+grp_size]\n",
    "    topk_idx   = np.argsort(grp_scores)[-3:]\n",
    "    # 只要正样本(label=1)在 top3 就算命中\n",
    "    hits.append(any(grp_labels[i]==1 for i in topk_idx))\n",
    "    offset += grp_size\n",
    "\n",
    "print(\"Ranker Test Top-3 Recall: \", np.mean(hits))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
